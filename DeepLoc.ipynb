{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLoc Mxnet Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import time\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet.initializer as init\n",
    "from mxnet import npx, autograd, optimizer, gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200                #-- integer, epoch\n",
    "batch_size = 32            #-- integer, minibatches size\n",
    "max_seq_size = 1000        #-- integer, maximum sequence size\n",
    "n_hid = 256                #-- integer, number of hidden neurons\n",
    "n_feat = 20                #-- integer, number of features encoded  X_test.shape[2]\n",
    "n_class = 10               #-- integer, number of classes to output\n",
    "lr = 0.0005                #-- float, learning rate\n",
    "drop_per  = 0.2            #-- float, input dropout\n",
    "drop_hid = 0.5             #-- float, hidden neurons dropout\n",
    "n_filt_1 = 20              #-- integer, number of filter in the first convolutional layer\n",
    "n_filt_2 = 128             #-- integer, number of filter in the second convolutional layer\n",
    "seed     = 123456          #-- seed\n",
    "loss_fn  = 'cross_entropy' #-- 'cross_entropy' or 'cosine' loss function\n",
    "test_data_set = 'large'    #-- 'large or small'\n",
    "train_data_set = 'large'   #-- 'large or small'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "mx.random.seed(seed)\n",
    "npx.random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def generate_run_id():\n",
    "    t = time.gmtime()\n",
    "    return \"{0}{1:0>2d}{2:0>2d}-{3:0>2d}{4:0>2d}\".format(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_data_set == 'large':\n",
    "    train_file = 'data/deeploc_full.npz'\n",
    "else:\n",
    "    train_file = 'subcellular_localization/data/train.npz'\n",
    "    \n",
    "if test_data_set == 'large':\n",
    "    test_file = 'data/deeploc_full.npz'\n",
    "else:\n",
    "    test_file = 'subcellular_localization/data/test.npz'\n",
    "\n",
    "train_npz = np.load(train_file)\n",
    "test_npz = np.load(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_train = nd.from_numpy(train_npz['mask_train']).as_in_context(ctx)\n",
    "#partition = nd.from_numpy(train_npz['partition']).as_in_context(ctx)\n",
    "#X_train = nd.from_numpy(train_npz['X_train']).as_in_context(ctx)\n",
    "#y_train = nd.from_numpy(train_npz['y_train']).as_in_context(ctx)\n",
    "#X_test = nd.from_numpy(test_npz['X_test']).as_in_context(ctx)\n",
    "#mask_test = nd.from_numpy(test_npz['mask_test']).as_in_context(ctx)\n",
    "#y_test = nd.from_numpy(test_npz['y_test']).as_in_context(ctx)\n",
    "\n",
    "mask_train = train_npz['mask_train']\n",
    "partition = train_npz['partition']\n",
    "X_train = train_npz['X_train']\n",
    "y_train = train_npz['y_train']\n",
    "X_test = test_npz['X_test']\n",
    "mask_test = test_npz['mask_test']\n",
    "y_test = test_npz['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz.close()\n",
    "test_npz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluted Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvLayer(nn.Block):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_conv_01 = nn.Conv1D(prefix='01_', channels=n_filt_1, kernel_size=1,  padding=0,  layout='NCW', activation='relu')\n",
    "            self.l_conv_03 = nn.Conv1D(prefix='03_', channels=n_filt_1, kernel_size=3,  padding=1,  layout='NCW', activation='relu')\n",
    "            self.l_conv_05 = nn.Conv1D(prefix='05_', channels=n_filt_1, kernel_size=5,  padding=2,  layout='NCW', activation='relu')\n",
    "            self.l_conv_09 = nn.Conv1D(prefix='09_', channels=n_filt_1, kernel_size=9,  padding=4,  layout='NCW', activation='relu')\n",
    "            self.l_conv_15 = nn.Conv1D(prefix='15_', channels=n_filt_1, kernel_size=15, padding=7,  layout='NCW', activation='relu')\n",
    "            self.l_conv_21 = nn.Conv1D(prefix='21_', channels=n_filt_1, kernel_size=21, padding=10, layout='NCW', activation='relu')\n",
    "            \n",
    "            self.l_conv_final = nn.Conv1D(prefix='conc_', channels=n_filt_2, kernel_size=3, padding=1, layout='NCW', activation='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.l_conv_01(x)\n",
    "        b = self.l_conv_03(x)\n",
    "        c = self.l_conv_05(x)\n",
    "        d = self.l_conv_09(x)\n",
    "        e = self.l_conv_15(x)\n",
    "        f = self.l_conv_21(x)\n",
    "        \n",
    "        conc = nd.concat(a, b, c, d, e, f, dim=1)\n",
    "        \n",
    "        return self.l_conv_final(conc)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BidirectionalLSTM(nn.Block):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BidirectionalLSTM, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_fwd = rnn.LSTM(hidden_size=n_hid, layout='TNC', prefix='Fwd_')\n",
    "            self.l_bck = rnn.LSTM(hidden_size=n_hid, layout='TNC', prefix='Bck_')\n",
    "\n",
    "    def _reverse(self, x, mask):\n",
    "        x_1 = nd.empty(x.shape, ctx=ctx)\n",
    "        nd.reset_arrays(x_1, num_arrays=1)\n",
    "        for i in range(x.shape[0]):\n",
    "            size = nd.sum(mask[i]).astype('int32').asscalar()\n",
    "            seq = x[i]\n",
    "            seq_1 = nd.reverse(nd.slice(seq, begin=(0,0), end=(size, n_filt_2)), axis=1)\n",
    "            seq_1.copyto(x_1[i, :size]) \n",
    "        return x_1            \n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        dimension = (1, 0, 2)\n",
    "        x_fwd = nd.transpose(x, dimension)\n",
    "        x_bck = nd.transpose(self._reverse(x, mask), dimension)\n",
    "        fwd = self.l_fwd(x_fwd)\n",
    "        bck = self.l_bck(x_bck)\n",
    "        conc = nd.concat(fwd, bck, dim=2)\n",
    "        return nd.transpose(conc, dimension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMAttentionDecodeFeedback(nn.Block):\n",
    "    def __init__(self,\n",
    "                 num_units,\n",
    "                 aln_num_units,\n",
    "                 n_decodesteps=10,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(LSTMAttentionDecodeFeedback, self).__init__(**kwargs)\n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.aln_num_units = aln_num_units\n",
    "        self.n_decodesteps = n_decodesteps\n",
    "        self.attention_softmax_function = nd.softmax\n",
    "        self.peepholes = True\n",
    "\n",
    "        self.num_inputs = 512\n",
    "        \n",
    "        self.nonlinearity_align=nd.tanh\n",
    "        \n",
    "        self.nonlinearity_ingate = nd.sigmoid\n",
    "        self.nonlinearity_forgetgate = nd.sigmoid\n",
    "        self.nonlinearity_cell = nd.tanh\n",
    "        self.nonlinearity_outgate = nd.sigmoid\n",
    "        \n",
    "        self.nonlinearity_out = nd.tanh\n",
    "        \n",
    "        self.W_hid_to_ingate = self.params.get('W_hid_to_ingate', shape=(num_units, num_units),\n",
    "                                               init=init.Normal(0.1),\n",
    "                                               allow_deferred_init=True)\n",
    "        \n",
    "        self.W_hid_to_forgetgate = self.params.get('W_hid_to_forgetgate', shape=(num_units, num_units),\n",
    "                                                   init=init.Normal(0.1),\n",
    "                                                   allow_deferred_init=True)\n",
    "        \n",
    "        self.W_hid_to_cell = self.params.get('W_hid_to_cell', shape=(num_units, num_units),\n",
    "                                             init=init.Normal(0.1),\n",
    "                                             allow_deferred_init=True)\n",
    "        \n",
    "        self.W_hid_to_outgate = self.params.get('W_hid_to_outgate', shape=(num_units, num_units),\n",
    "                                                init=init.Normal(0.1),\n",
    "                                                allow_deferred_init=True)\n",
    "        \n",
    "        self.b_ingate = self.params.get('b_ingate', shape=(num_units),\n",
    "                                        init=init.Constant(0),\n",
    "                                        allow_deferred_init=True)\n",
    "\n",
    "        self.b_forgetgate = self.params.get('b_forgetgate', shape=(num_units),\n",
    "                                            init=init.Constant(0),\n",
    "                                            allow_deferred_init=True)\n",
    "\n",
    "        self.b_cell = self.params.get('b_cell', shape=(num_units),\n",
    "                                      init=init.Constant(0),\n",
    "                                      allow_deferred_init=True)\n",
    "        \n",
    "        self.b_outgate = self.params.get('b_outgate', shape=(num_units),\n",
    "                                         init=init.Constant(0),\n",
    "                                         allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_ingate = self.params.get('W_weightedhid_to_ingate',\n",
    "                                                      shape=(self.num_inputs, num_units),\n",
    "                                                      init=init.Normal(0.1),\n",
    "                                                      allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_forgetgate = self.params.get('W_weightedhid_to_forgetgate',\n",
    "                                                           shape=(self.num_inputs, num_units),\n",
    "                                                           init=init.Normal(0.1),\n",
    "                                                           allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_cell = self.params.get('W_weightedhid_to_cell',\n",
    "                                                     shape=(self.num_inputs, num_units),\n",
    "                                                     init=init.Normal(0.1),\n",
    "                                                     allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_outgate = self.params.get('W_weightedhid_to_outgate',\n",
    "                                                        shape=(self.num_inputs, num_units),\n",
    "                                                        init=init.Normal(0.1),\n",
    "                                                        allow_deferred_init=True)\n",
    "        \n",
    "        self.W_cell_to_ingate = self.params.get('W_cell_to_ingate',\n",
    "                                                shape=(num_units),\n",
    "                                                init=init.Normal(0.1),\n",
    "                                                allow_deferred_init=True)\n",
    "        \n",
    "        self.W_cell_to_forgetgate = self.params.get('W_cell_to_forgetgate',\n",
    "                                                    shape=(num_units),\n",
    "                                                    init=init.Normal(0.1),\n",
    "                                                    allow_deferred_init=True)\n",
    "        \n",
    "        self.W_cell_to_outgate = self.params.get('W_cell_to_outgate',\n",
    "                                                 shape=(num_units),\n",
    "                                                 init=init.Normal(0.1),\n",
    "                                                 allow_deferred_init=True)\n",
    "        \n",
    "        self.W_align = self.params.get('W_align',\n",
    "                                       shape=(num_units, self.aln_num_units),\n",
    "                                       init=init.Normal(0.1))\n",
    "        \n",
    "        self.U_align = self.params.get('U_align', shape=(self.num_inputs,self.aln_num_units),\n",
    "                                       init=init.Normal(0.1),\n",
    "                                       allow_deferred_init=True)\n",
    "        \n",
    "        self.v_align = self.params.get('v_align', shape=(self.aln_num_units, 1),\n",
    "                                       init=init.Normal(0.1))\n",
    "        \n",
    "        with self.name_scope():\n",
    "            pass\n",
    "\n",
    "    def slice_w(self, x, n):\n",
    "        return x[:, n*self.num_units:(n+1)*self.num_units]\n",
    "    \n",
    "    def step(self, cell_previous, hid_previous, alpha_prev, weighted_hidden_prev,\n",
    "            input, mask, hUa, W_align, v_align,\n",
    "            W_hid_stacked, W_weightedhid_stacked, W_cell_to_ingate,\n",
    "            W_cell_to_forgetgate, W_cell_to_outgate,\n",
    "            b_stacked, *args):\n",
    "        \n",
    "        sWa = nd.dot(hid_previous, W_align)  # (BS, aln_num_units)\n",
    "        sWa = nd.expand_dims(sWa, axis=1)    # (BS, 1 aln_num_units) \n",
    "        align_act = sWa + hUa\n",
    "        tanh_sWahUa = nd.tanh(align_act)     # (BS, seqlen, num_units_aln)\n",
    "        \n",
    "        # CALCULATE WEIGHT FOR EACH HIDDEN STATE VECTOR\n",
    "        a = nd.dot(tanh_sWahUa, v_align)  # (BS, Seqlen, 1)\n",
    "        a = nd.reshape(a, (a.shape[0], a.shape[1]))\n",
    "        #                                # (BS, Seqlen)\n",
    "        # # ->(BS, seq_len)\n",
    "        \n",
    "        a = a*mask - (1-mask)*10000\n",
    "        \n",
    "        alpha = self.attention_softmax_function(a)\n",
    "        \n",
    "        # input: (BS, Seqlen, num_units)\n",
    "        weighted_hidden = input * nd.expand_dims(alpha, axis=2)\n",
    "        weighted_hidden = nd.sum(weighted_hidden, axis=1)  #sum seqlen out\n",
    "\n",
    "        # (BS, dec_hid) x (dec_hid, dec_hid)\n",
    "        gates = nd.dot(hid_previous, W_hid_stacked) + b_stacked\n",
    "        # (BS, enc_hid) x (enc_hid, dec_hid)\n",
    "        gates = gates + nd.dot(weighted_hidden, W_weightedhid_stacked)\n",
    "\n",
    "        \n",
    "        # Clip gradients\n",
    "        # if self.grad_clipping is not False:\n",
    "        #    gates = theano.gradient.grad_clip(\n",
    "        #        gates, -self.grad_clipping, self.grad_clipping)\n",
    "\n",
    "        # Extract the pre-activation gate values\n",
    "        ingate = self.slice_w(gates, 0)\n",
    "        forgetgate = self.slice_w(gates, 1)\n",
    "        cell_input = self.slice_w(gates, 2)\n",
    "        outgate = self.slice_w(gates, 3)\n",
    "\n",
    "        if self.peepholes:\n",
    "            # Compute peephole connections\n",
    "            ingate = ingate + cell_previous*W_cell_to_ingate\n",
    "            forgetgate = forgetgate + (cell_previous*W_cell_to_forgetgate)\n",
    "            \n",
    "        # Apply nonlinearities\n",
    "        ingate = self.nonlinearity_ingate(ingate)\n",
    "        forgetgate = self.nonlinearity_forgetgate(forgetgate)\n",
    "        cell_input = self.nonlinearity_cell(cell_input)\n",
    "        outgate = self.nonlinearity_outgate(outgate)\n",
    "        \n",
    "        # Compute new cell value\n",
    "        cell = forgetgate*cell_previous + ingate*cell_input\n",
    "        \n",
    "        if self.peepholes:\n",
    "            outgate = outgate + cell*W_cell_to_outgate\n",
    "\n",
    "        # W_align:  (num_units, aln_num_units)\n",
    "        # U_align:  (num_feats, aln_num_units)\n",
    "        # v_align:  (aln_num_units, 1)\n",
    "        # hUa:      (BS, Seqlen, aln_num_units)\n",
    "        # hid:      (BS, num_units_dec)\n",
    "        # input:    (BS, Seqlen, num_inputs)\n",
    "\n",
    "        # Compute new hidden unit activation\n",
    "        hid = outgate*self.nonlinearity_out(cell)\n",
    "\n",
    "        return [cell, hid, alpha, weighted_hidden]            \n",
    "            \n",
    "        \n",
    "    def forward(self, input, mask):\n",
    "        \n",
    "        num_batch = input.shape[0]\n",
    "        encode_seqlen = input.shape[1]\n",
    "        \n",
    "        W_hid_stacked = nd.concat(\n",
    "            self.W_hid_to_ingate.data(),\n",
    "            self.W_hid_to_forgetgate.data(),\n",
    "            self.W_hid_to_cell.data(),\n",
    "            self.W_hid_to_outgate.data(),\n",
    "            dim=1)\n",
    "        \n",
    "        W_weightedhid_stacked = nd.concat(\n",
    "            self.W_weightedhid_to_ingate.data(),\n",
    "            self.W_weightedhid_to_forgetgate.data(),\n",
    "            self.W_weightedhid_to_cell.data(),\n",
    "            self.W_weightedhid_to_outgate.data(),\n",
    "            dim=1)\n",
    "        \n",
    "        b_stacked = nd.concat(\n",
    "            self.b_ingate.data(),\n",
    "            self.b_forgetgate.data(),\n",
    "            self.b_cell.data(),\n",
    "            self.b_outgate.data(),\n",
    "            dim=0)\n",
    "        \n",
    "        cell = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        hid = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        alpha = nd.zeros((num_batch, encode_seqlen), ctx=ctx)\n",
    "        weighted_hidden = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        \n",
    "        hUa = nd.dot(input, self.U_align.data())\n",
    "        W_align = self.W_align.data()\n",
    "        v_align = self.v_align.data()\n",
    "        \n",
    "        W_cell_to_ingate = self.W_cell_to_ingate.data()\n",
    "        W_cell_to_forgetgate = self.W_cell_to_forgetgate.data()\n",
    "        W_cell_to_outgate = self.W_cell_to_outgate.data()\n",
    "        \n",
    "        for i in range(self.n_decodesteps):        \n",
    "            cell, hid, alpha, weighted_hidden = self.step(cell, hid, alpha, weighted_hidden,\n",
    "                input, mask, hUa, W_align, v_align,\n",
    "                W_hid_stacked, W_weightedhid_stacked, W_cell_to_ingate,\n",
    "                W_cell_to_forgetgate, W_cell_to_outgate,\n",
    "                b_stacked)\n",
    "        \n",
    "        return weighted_hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLoc Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_dropout_1 = nn.Dropout(rate=drop_per)\n",
    "            self.l_dropout_2 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_dropout_3 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_dropout_4 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_conv = ConvLayer(prefix='Conv_')\n",
    "            self.l_lstm = BidirectionalLSTM(prefix='BLSTM_')\n",
    "            self.l_dense = nn.Dense(units=n_class, activation='relu')\n",
    "            self.l_decoder = LSTMAttentionDecodeFeedback(\n",
    "                              prefix='Decoder_',\n",
    "                              num_units=2*n_hid, aln_num_units=n_hid, n_decodesteps=10)\n",
    "    \n",
    "    def forward(self, input, mask):\n",
    "        x = self.l_dropout_1.forward(input)\n",
    "        x = nd.transpose(x, (0, 2, 1))\n",
    "        x = self.l_conv.forward(x)\n",
    "        x = nd.transpose(x, (0, 2, 1))\n",
    "        x = self.l_dropout_2.forward(x)\n",
    "        x = self.l_lstm.forward(x, mask)\n",
    "        x = self.l_decoder(x, mask)\n",
    "        x = self.l_dropout_3.forward(x)\n",
    "        x = self.l_dense.forward(x)\n",
    "        x = self.l_dropout_4.forward(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConfusionMatrix:\n",
    "    \"\"\"\n",
    "       Simple confusion matrix class\n",
    "       row is the true class, column is the predicted class\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, class_names=None):\n",
    "        self.n_classes = num_classes\n",
    "        if class_names is None:\n",
    "            self.class_names = map(str, range(num_classes))\n",
    "        else:\n",
    "            self.class_names = class_names\n",
    "\n",
    "        # find max class_name and pad\n",
    "        max_len = max(map(len, self.class_names))\n",
    "        self.max_len = max_len\n",
    "        for idx, name in enumerate(self.class_names):\n",
    "            if len(self.class_names) < max_len:\n",
    "                self.class_names[idx] = name + \" \"*(max_len-len(name))\n",
    "\n",
    "        self.mat = np.zeros((num_classes,num_classes),dtype='int')\n",
    "\n",
    "    def __str__(self):\n",
    "        # calucate row and column sums\n",
    "        col_sum = np.sum(self.mat, axis=1)\n",
    "        row_sum = np.sum(self.mat, axis=0)\n",
    "\n",
    "        s = []\n",
    "\n",
    "        mat_str = self.mat.__str__()\n",
    "        mat_str = mat_str.replace('[','').replace(']','').split('\\n')\n",
    "\n",
    "        for idx, row in enumerate(mat_str):\n",
    "            if idx == 0:\n",
    "                pad = \" \"\n",
    "            else:\n",
    "                pad = \"\"\n",
    "            class_name = self.class_names[idx]\n",
    "            class_name = \" \" + class_name + \" |\"\n",
    "            row_str = class_name + pad + row\n",
    "            row_str += \" |\" + str(col_sum[idx])\n",
    "            s.append(row_str)\n",
    "\n",
    "        row_sum = [(self.max_len+4)*\" \"+\" \".join(map(str, row_sum))]\n",
    "        hline = [(1+self.max_len)*\" \"+\"-\"*len(row_sum[0])]\n",
    "\n",
    "        s = hline + s + hline + row_sum\n",
    "\n",
    "        # add linebreaks\n",
    "        s_out = [line+'\\n' for line in s]\n",
    "        return \"\".join(s_out)\n",
    "\n",
    "    def batch_add(self, targets, preds):\n",
    "        assert targets.shape == preds.shape\n",
    "        assert len(targets) == len(preds)\n",
    "        assert max(targets) < self.n_classes\n",
    "        assert max(preds) < self.n_classes\n",
    "        targets = targets.flatten()\n",
    "        preds = preds.flatten()\n",
    "        for i in range(len(targets)):\n",
    "            self.mat[targets[i], preds[i]] += 1\n",
    "    def ret_mat(self):\n",
    "        return self.mat\n",
    "\n",
    "    def get_errors(self):\n",
    "        tp = np.asarray(np.diag(self.mat).flatten(),dtype='float')\n",
    "        fn = np.asarray(np.sum(self.mat, axis=1).flatten(),dtype='float') - tp\n",
    "        fp = np.asarray(np.sum(self.mat, axis=0).flatten(),dtype='float') - tp\n",
    "        tn = np.asarray(np.sum(self.mat)*np.ones(self.n_classes).flatten(),\n",
    "                        dtype='float') - tp - fn - fp\n",
    "        return tp, fn, fp, tn\n",
    "\n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        Calculates global accuracy\n",
    "        :return: accuracy\n",
    "        :example: >>> conf = ConfusionMatrix(3)\n",
    "                  >>> conf.batchAdd([0,0,1],[0,0,2])\n",
    "                  >>> print conf.accuracy()\n",
    "        \"\"\"\n",
    "        tp, _, _, _ = self.get_errors()\n",
    "        n_samples = np.sum(self.mat)\n",
    "        return np.sum(tp) / n_samples\n",
    "\n",
    "    def sensitivity(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = tp / (tp + fn)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def specificity(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = tn / (tn + fp)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def positive_predictive_value(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = tp / (tp + fp)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def negative_predictive_value(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = tn / (tn + fn)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def false_positive_rate(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = fp / (fp + tn)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def false_discovery_rate(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = fp / (tp + fp)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def F1(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        res = (2*tp) / (2*tp + fp + fn)\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "\n",
    "    def matthews_correlation(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        numerator = tp*tn - fp*fn\n",
    "        denominator = np.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))\n",
    "        res = numerator / denominator\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res\n",
    "    def OMCC(self):\n",
    "        tp, tn, fp, fn = self.get_errors()\n",
    "        tp = np.sum(tp)\n",
    "        tn = np.sum(tn)\n",
    "        fp = np.sum(fp)\n",
    "        fn = np.sum(fn)\n",
    "        numerator = tp*tn - fp*fn\n",
    "        denominator = np.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))\n",
    "        res = numerator / denominator\n",
    "        res = res[~np.isnan(res)]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "run_id = generate_run_id()\n",
    "print(run_id)\n",
    "\n",
    "hparams_path='./models/{}_hparams.json'.format(run_id)\n",
    "net_params_path='./models/{}.params'.format(run_id)\n",
    "logdir='./logs/{}/{}'.format(train_data_set, run_id)\n",
    "import json\n",
    "\n",
    "dict = {'run_id' : run_id,\n",
    "        'epoch' : epoch,\n",
    "        'batch_size' : batch_size,\n",
    "        'n_hid' : n_hid,\n",
    "        'n_feat' : n_feat,\n",
    "        'n_class' : n_class,\n",
    "        'lr' : lr,\n",
    "        'drop_per' : drop_per,\n",
    "        'drop_hid' : drop_hid,\n",
    "        'n_filt_1' : n_filt_1,\n",
    "        'n_filt_2' : n_filt_2,\n",
    "        'seed' : seed,\n",
    "        'loss_fn' : loss_fn,\n",
    "        'train_data_set' : train_data_set}\n",
    "\n",
    "\n",
    "json = json.dumps(dict)\n",
    "f = open(hparams_path,\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "sw = SummaryWriter(logdir=logdir)\n",
    "    \n",
    "\n",
    "if (loss_fn == 'cosine'):\n",
    "  loss_function = gluon.loss.CosineEmbeddingLoss()  \n",
    "elif (loss_fn == 'cross_entropy'):\n",
    "  loss_function =  gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "net = Model(prefix='net_')\n",
    "net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "params = net.collect_params()\n",
    "params.reset_ctx([ctx])\n",
    "\n",
    "trainer = gluon.Trainer(params=params,\n",
    "                        optimizer='adam', optimizer_params={'learning_rate':lr})\n",
    "\n",
    "for p in range(1, 5):\n",
    "    \n",
    "    # Train and validation sets\n",
    "    train_index = np.where(partition != p)\n",
    "    val_index = np.where(partition == p)\n",
    "    X_tr = nd.from_numpy(X_train[train_index].astype(np.float32)).as_in_context(ctx)\n",
    "    X_val = nd.from_numpy(X_train[val_index].astype(np.float32)).as_in_context(ctx)\n",
    "    y_tr = nd.from_numpy(y_train[train_index].astype(np.int32)).as_in_context(ctx)\n",
    "    y_val = nd.from_numpy(y_train[val_index].astype(np.int32)).as_in_context(ctx)\n",
    "    mask_tr = nd.from_numpy(mask_train[train_index].astype(np.float32)).as_in_context(ctx)\n",
    "    mask_val = nd.from_numpy(mask_train[val_index].astype(np.float32)).as_in_context(ctx)\n",
    "    \n",
    "    train_iter = mx.io.NDArrayIter([X_tr, mask_tr], y_tr, batch_size, shuffle=True)\n",
    "    val_iter = mx.io.NDArrayIter([X_val, mask_val], y_val, batch_size, shuffle=False)\n",
    "    \n",
    "    eps = []\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for e in range(1, epoch + 1):\n",
    "        step = ((p - 1) * epoch) + e\n",
    "        begin_time = time.perf_counter()\n",
    "        train_loss = 0.\n",
    "        train_acc = mx.metric.Accuracy()\n",
    "        train_iter.reset()\n",
    "        val_iter.reset()\n",
    "        \n",
    "        # Full pass training set\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        confusion_train = ConfusionMatrix(n_class)\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            input = batch.data[0]\n",
    "            mask = batch.data[1]\n",
    "            label = batch.label[0]\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                output = net(input, mask)\n",
    "\n",
    "                if (loss_fn == 'cosine'):\n",
    "                    # Cosine loss\n",
    "                    l = loss_function(output, nd.one_hot(label, n_class), nd.array([1.0], ctx=ctx))\n",
    "                else:\n",
    "                    # Softmax cross entropy\n",
    "                    l = loss_function(output, label)\n",
    "                    \n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            train_err += l.mean().asscalar()\n",
    "            preds = output.argmax(axis=1)\n",
    "            train_acc.update(label, preds)\n",
    "            train_batches += 1\n",
    "            np_label = label.astype('int32').asnumpy()\n",
    "            np_preds = preds.astype('int32').asnumpy()\n",
    "            confusion_train.batch_add(np_label, np_preds)\n",
    "\n",
    "        stop_time = time.perf_counter()\n",
    "        train_time = stop_time - begin_time\n",
    "        # train_acc = train_acc.get()[1] \n",
    "        # avg_train_loss = train_loss/train_batches\n",
    "\n",
    "        train_loss = train_err / train_batches\n",
    "        train_accuracy = confusion_train.accuracy()\n",
    "        cf_train = confusion_train.ret_mat()\n",
    "        \n",
    "        sw.add_scalar(tag='train_time', value=train_time, global_step=step)\n",
    "        sw.add_scalar(tag='train_loss', value=train_loss, global_step=step)\n",
    "        sw.add_scalar(tag='train_accuracy', value=train_accuracy, global_step=step)\n",
    "\n",
    "        param_names = net.collect_params().keys()\n",
    "        grads = [i.grad() for i in net.collect_params().values()]\n",
    "        assert len(grads) == len(param_names)\n",
    "        # logging the gradients of parameters for checking convergence\n",
    "        for i, name in enumerate(param_names):\n",
    "            sw.add_histogram(tag=name, values=grads[i], global_step=step, bins=1000)\n",
    "\n",
    "        print(\"%d,%.5f,%.5f,%.5f\" % (e, train_time, train_accuracy, train_loss))\n",
    "\n",
    "        # Full pass validation set\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        val_acc = mx.metric.Accuracy()\n",
    "        confusion_valid = ConfusionMatrix(n_class)\n",
    "\n",
    "        for batch in val_iter:\n",
    "            input = batch.data[0]\n",
    "            mask = batch.data[1]\n",
    "            label = batch.label[0]\n",
    "\n",
    "            with mx.autograd.predict_mode():\n",
    "                output = net(input, mask)\n",
    "\n",
    "                if (loss_fn == 'cosine'):\n",
    "                    # Cosine loss\n",
    "                    l = loss_function(output, nd.one_hot(label, n_class), nd.array([1.0], ctx=ctx))\n",
    "                else:\n",
    "                    # Softmax cross entropy\n",
    "                    l = loss_function(output, label)\n",
    "                \n",
    "            preds = output.argmax(axis=1)\n",
    "            np_label = label.asnumpy()\n",
    "            val_acc.update(label, preds)\n",
    "            np_preds = preds.astype('int32').asnumpy()\n",
    "            confusion_valid.batch_add(np_label, np_preds)\n",
    "            val_batches += 1\n",
    "            val_err += l.mean().asscalar()\n",
    "            \n",
    "        val_loss = val_err / val_batches\n",
    "        val_accuracy = confusion_valid.accuracy()\n",
    "        cf_val = confusion_valid.ret_mat()\n",
    "\n",
    "        sw.add_scalar(tag='val_loss', value=val_loss, global_step=step)\n",
    "        sw.add_scalar(tag='val_accuracy', value=val_accuracy, global_step=step)\n",
    "        \n",
    "        f_val_acc = val_accuracy\n",
    "            \n",
    "        # Full pass test set if validation accuracy is higher\n",
    "        if f_val_acc >= best_val_acc:\n",
    "            test_batches = 0\n",
    "            \n",
    "            confusion_test = ConfusionMatrix(n_class)\n",
    "            \n",
    "            mask_nd = nd.from_numpy(mask_test.astype(np.float32)).as_in_context(ctx)\n",
    "            X_nd = nd.from_numpy(X_test.astype(np.float32)).as_in_context(ctx)\n",
    "            y_nd = nd.from_numpy(y_test.astype(np.float32)).as_in_context(ctx)\n",
    "    \n",
    "            test_iter = mx.io.NDArrayIter([X_nd, mask_nd], y_nd, batch_size, shuffle=False)\n",
    "            \n",
    "            for batch in test_iter:\n",
    "                input = batch.data[0]\n",
    "                mask = batch.data[1]\n",
    "                label = batch.label[0]\n",
    "\n",
    "                with mx.autograd.predict_mode():\n",
    "                    output = net(input, mask)\n",
    "\n",
    "                preds = output.argmax(axis=1)\n",
    "                np_label = label.astype('int32').asnumpy()\n",
    "                np_preds = preds.astype('int32').asnumpy()\n",
    "                confusion_test.batch_add(np_label, np_preds)\n",
    "            \n",
    "            print(confusion_test.accuracy())\n",
    "            print(confusion_test.ret_mat())\n",
    "\n",
    "            best_val_acc = f_val_acc\n",
    "        \n",
    "            net.save_parameters(net_params_path)\n",
    "\n",
    "sw.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small - cross entropy\n",
    "# 20200705-02-57.params\n",
    "\n",
    "# full - cosine\n",
    "# 20200802-0216.params\n",
    "\n",
    "net_params_path = 'models/20200802-0216.params'\n",
    "\n",
    "mask = nd.from_numpy(mask_test.astype(np.float32)).as_in_context(ctx)\n",
    "X    = nd.from_numpy(X_test.astype(np.float32)).as_in_context(ctx)\n",
    "y    = nd.from_numpy(y_test.astype(np.int32)).as_in_context(ctx)\n",
    "\n",
    "test_iter = mx.io.NDArrayIter([X, mask], y, 5, shuffle=False)\n",
    "\n",
    "net = Model(prefix='net_')\n",
    "net.load_parameters(net_params_path, ctx=ctx)\n",
    "\n",
    "confusion_test = ConfusionMatrix(10)\n",
    "\n",
    "for batch in test_iter:\n",
    "    input = batch.data[0]\n",
    "    mask = batch.data[1]\n",
    "    label = batch.label[0]\n",
    "\n",
    "    with mx.autograd.predict_mode():\n",
    "        output = net(input, mask)\n",
    "        \n",
    "    preds = output.argmax(axis=1)\n",
    "    np_label = label.asnumpy()\n",
    "    np_preds = preds.astype('int32').asnumpy()\n",
    "    confusion_test.batch_add(np_label, np_preds)\n",
    "\n",
    "print(\"Accuracy: %.5f\" % (confusion_test.accuracy()))\n",
    "print(confusion_test.ret_mat())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
