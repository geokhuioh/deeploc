{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLoc Mxnet Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy\n",
    "import time\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet.initializer as init\n",
    "from mxnet import np, npx, autograd, optimizer, gluon\n",
    "from mxnet.gluon import nn, rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200         #-- integer, epoch\n",
    "batch_size = 32     #-- integer, minibatches size\n",
    "max_seq_size = 1000 #-- integer, maximum sequence size\n",
    "n_hid = 256         #-- integer, number of hidden neurons\n",
    "n_feat = 20         #-- integer, number of features encoded  X_test.shape[2]\n",
    "n_class = 10        #-- integer, number of classes to output\n",
    "lr = 0.0010         #-- float, learning rate\n",
    "drop_per  = 0.2     #-- float, input dropout\n",
    "drop_hid = 0.5      #-- float, hidden neurons dropout\n",
    "n_filt = 10         #-- integer, number of filter in the first convolutional layer\n",
    "seed = 123456       #-- seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "mx.random.seed(seed)\n",
    "npx.random.seed(seed)\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'subcellular_localization/data/train.npz'\n",
    "test_file = 'subcellular_localization/data/test.npz'\n",
    "\n",
    "train_npz = numpy.load(train_file)\n",
    "test_npz = numpy.load(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = nd.from_numpy(train_npz['mask_train'])\n",
    "partition = nd.from_numpy(train_npz['partition'])\n",
    "X_train = nd.from_numpy(train_npz['X_train'])\n",
    "y_train = nd.from_numpy(train_npz['y_train'])\n",
    "X_test = nd.from_numpy(test_npz['X_test'])\n",
    "mask_test = nd.from_numpy(test_npz['mask_test'])\n",
    "y_test = nd.from_numpy(test_npz['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz.close()\n",
    "test_npz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dropout = nn.Dropout(rate=drop_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Shuffle Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dimension Shuffle Layer\n",
    "    \n",
    "class DimShuffle(nn.Block):\n",
    "    def __init__(self, order, **kwargs):\n",
    "        super(DimShuffle, self).__init__(**kwargs)\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mx.nd.transpose(x, self.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dimshuffle = DimShuffle(order = (0, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluted Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_conv_a = lasagne.layers.Conv1DLayer(l_shu, num_filters=n_filt, pad='same', stride=1,W=w_inits, filter_size=f_size_a, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "# l_conv_b = lasagne.layers.Conv1DLayer(l_shu, num_filters=n_filt, pad='same', stride=1,W=w_inits, filter_size=f_size_b, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "# l_conv_c = lasagne.layers.Conv1DLayer(l_shu, num_filters=n_filt, pad='same', stride=1,W=w_inits, filter_size=f_size_c, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "# l_conv_d = lasagne.layers.Conv1DLayer(l_shu, num_filters=n_filt, pad='same', stride=1,W=w_inits, filter_size=f_size_d, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "# l_conv_e = lasagne.layers.Conv1DLayer(l_shu, num_filters=n_filt, pad='same', stride=1,W=w_inits, filter_size=f_size_e, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "# l_conv_f = lasagne.layers.Conv1DLayer(l_shu, num_filters=n_filt, pad='same', stride=1,W=w_inits, filter_size=f_size_f, nonlinearity=lasagne.nonlinearities.rectify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvLayer, self).__init__(**kwargs)\n",
    "        f_size_a = 1\n",
    "        f_size_b = 3\n",
    "        f_size_c = 5\n",
    "        f_size_d = 9\n",
    "        f_size_e = 15\n",
    "        f_size_f = 21\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.l_conv_a = nn.Conv1D(channels=n_filt, kernel_size=f_size_a, padding=0, layout='NCW', activation='relu')\n",
    "            self.l_conv_b = nn.Conv1D(channels=n_filt, kernel_size=f_size_b, padding=1, layout='NCW', activation='relu')\n",
    "            self.l_conv_c = nn.Conv1D(channels=n_filt, kernel_size=f_size_c, padding=2, layout='NCW', activation='relu')\n",
    "            self.l_conv_d = nn.Conv1D(channels=n_filt, kernel_size=f_size_d, padding=4, layout='NCW', activation='relu')\n",
    "            self.l_conv_e = nn.Conv1D(channels=n_filt, kernel_size=f_size_e, padding=7, layout='NCW', activation='relu')\n",
    "            self.l_conv_f = nn.Conv1D(channels=n_filt, kernel_size=f_size_f, padding=10, layout='NCW', activation='relu')\n",
    "            \n",
    "            self.l_conv_final = nn.Conv1D(channels=64, kernel_size=f_size_b, padding=1, layout='NCW', activation='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.l_conv_a(x)\n",
    "        b = self.l_conv_b(x)\n",
    "        c = self.l_conv_c(x)\n",
    "        d = self.l_conv_d(x)\n",
    "        e = self.l_conv_e(x)\n",
    "        f = self.l_conv_f(x)\n",
    "        \n",
    "        conc = nd.concat(a, b, c, d, e, f, dim=1)\n",
    "        \n",
    "        return self.l_conv_final(conc)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_fwd = lasagne.layers.LSTMLayer(l_indrop, num_units=n_hid, name='LSTMFwd', mask_input=l_mask, cell_init=lasagne.init.Orthogonal(), hid_init=lasagne.init.Orthogonal(), nonlinearity=lasagne.nonlinearities.tanh, grad_clipping=2)\n",
    "# l_bck = lasagne.layers.LSTMLayer(l_indrop, num_units=n_hid, name='LSTMBck', mask_input=l_mask, cell_init=lasagne.init.Orthogonal(), hid_init=lasagne.init.Orthogonal(),\tbackwards=True, nonlinearity=lasagne.nonlinearities.tanh, grad_clipping=2)\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BidirectionalLSTM, self).__init__(**kwargs)\n",
    "        \n",
    "        # self.params.get('mask', shape=(batch_size, max_seq_size))\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_fwd = rnn.LSTM(hidden_size=n_hid, layout='TNC', prefix='LSTMFwd_')\n",
    "            self.l_bck = rnn.LSTM(hidden_size=n_hid, layout='TNC', prefix='LSTMBck_')\n",
    "\n",
    "    def _reverse(self, x, mask):\n",
    "        d = [x, mask]\n",
    "        dataiter = mx.io.NDArrayIter(d)\n",
    "        x_1 = nd.empty(x.shape, ctx=ctx)\n",
    "        nd.reset_arrays(x_1, num_arrays=1)\n",
    "        for i in range(x.shape[0]):\n",
    "            size = nd.sum(mask[i]).astype('int32').asscalar()\n",
    "            # print('size:', size)\n",
    "            seq = x[i]\n",
    "            seq_1 = nd.reverse(nd.slice(seq, begin=(0,0), end=(size, 64)), axis=1)\n",
    "            # print(\"seq_1: %s\", seq_1.shape)\n",
    "            # print(\"seq : %s\", seq.shape)\n",
    "            seq_1.copyto(x_1[i, :size]) \n",
    "        return x_1            \n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        # print('x shape:', x.shape)\n",
    "        # mask = self.params.get('mask').data()\n",
    "        # print('mask:', mask)\n",
    "        dimension = (1, 0, 2)\n",
    "        x_fwd = nd.transpose(x, dimension)\n",
    "        x_bck = nd.transpose(self._reverse(x, mask), dimension)\n",
    "        fwd = self.l_fwd(x_fwd)\n",
    "        bck = self.l_bck(x_bck)\n",
    "        conc = nd.concat(fwd, bck, dim=2)\n",
    "        return nd.transpose(conc, dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicerLayer(nn.Block):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(SlicerLayer, self).__init__(**kwargs)\n",
    "        # self.params.get('mask', shape=(128, 1000))\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # mask = self.params.get('mask').data()\n",
    "        d = [x, mask]\n",
    "        dataiter = mx.io.NDArrayIter(d)\n",
    "        x_shape = x.shape\n",
    "        x_1_shape = (x.shape[0], x.shape[1])\n",
    "        x_1 = nd.empty(x_1_shape, ctx=ctx)\n",
    "        nd.reset_arrays(x_1, num_arrays=1)\n",
    "        for i in range(x_1_shape[0]):\n",
    "            size = nd.sum(mask[i]).astype('int32').asscalar()\n",
    "            x[i][size - 1].copyto(x_1[i])\n",
    "        return x_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_shuffler(dimension):\n",
    "    def shuffler(x):\n",
    "        return nd.transpose(x, dimension)\n",
    "    return shuffler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = nn.Sequential()\n",
    "# with net.name_scope():\n",
    "#    net.add(nn.Dropout(rate=drop_per))\n",
    "#    net.add(nn.Lambda(dimension_shuffler((0, 2, 1))))\n",
    "#    net.add(l_conv)\n",
    "#    net.add(nn.Lambda(dimension_shuffler((0, 2, 1))))\n",
    "#    net.add(nn.Dropout(rate=drop_hid))\n",
    "#    net.add(nn.Lambda(dimension_shuffler((1, 0, 2))))\n",
    "#     net.add(rnn.LSTM(hidden_size=n_hid, layout='TNC', bidirectional=True, prefix='LSTM'))\n",
    "#    net.add(l_lstm)\n",
    "    # net.add(nn.Lambda(lambda x: x[-1]))\n",
    "#    net.add(SlicerLayer())\n",
    "#    net.add(nn.Dense(units=n_class, activation='relu'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionDecodeFeedback(nn.Block):\n",
    "    def __init__(self,\n",
    "                 num_units,\n",
    "                 aln_num_units,\n",
    "                 n_decodesteps=10,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(LSTMAttentionDecodeFeedback, self).__init__(prefix='LSTMAttentinDecode_Feedback_', **kwargs)\n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.aln_num_units = aln_num_units\n",
    "        self.n_decodesteps = n_decodesteps\n",
    "        self.attention_softmax_function = nd.softmax\n",
    "        self.peepholes = True\n",
    "\n",
    "        self.num_inputs = 512\n",
    "        \n",
    "        self.nonlinearity_align=nd.tanh\n",
    "        \n",
    "        self.nonlinearity_ingate = nd.sigmoid\n",
    "        self.nonlinearity_forgetgate = nd.sigmoid\n",
    "        self.nonlinearity_cell = nd.tanh\n",
    "        self.nonlinearity_outgate = nd.sigmoid\n",
    "        \n",
    "        self.nonlinearity_out = nd.tanh\n",
    "        \n",
    "        # self.params.get('mask', shape=(batch_size, max_seq_size))\n",
    "        self.params.get('W_hid_to_ingate', shape=(num_units, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('b_ingate', shape=(num_units), init=init.Constant(0))\n",
    "        self.params.get('W_hid_to_forgetgate', shape=(num_units, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('b_forgetgate', shape=(num_units), init=init.Constant(0))\n",
    "        self.params.get('W_hid_to_cell', shape=(num_units, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('b_cell', shape=(num_units), init=init.Constant(0))\n",
    "        self.params.get('W_hid_to_outgate', shape=(num_units, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('b_outgate', shape=(num_units), init=init.Constant(0))\n",
    "        \n",
    "        self.params.get('W_weightedhid_to_ingate', shape=(self.num_inputs, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('W_weightedhid_to_forgetgate', shape=(self.num_inputs, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('W_weightedhid_to_cell', shape=(self.num_inputs, num_units), init=init.Normal(0.1))\n",
    "        self.params.get('W_weightedhid_to_outgate', shape=(self.num_inputs, num_units), init=init.Normal(0.1))\n",
    "        \n",
    "        self.params.get('W_cell_to_ingate', shape=(num_units), init=init.Normal(0.1))\n",
    "        self.params.get('W_cell_to_forgetgate', shape=(num_units), init=init.Normal(0.1))\n",
    "        self.params.get('W_cell_to_outgate', shape=(num_units), init=init.Normal(0.1))\n",
    "        \n",
    "        self.params.get('W_align', shape=(num_units, self.aln_num_units), init=init.Normal(0.1))\n",
    "        self.params.get('U_align', shape=(self.num_inputs, self.aln_num_units), init=init.Normal(0.1))\n",
    "        self.params.get('v_align', shape=(self.aln_num_units, 1), init=init.Normal(0.1))\n",
    "        \n",
    "        with self.name_scope():\n",
    "            pass\n",
    "\n",
    "    def slice_w(self, x, n):\n",
    "        return x[:, n*self.num_units:(n+1)*self.num_units]\n",
    "    \n",
    "    def step(self, cell_previous, hid_previous, alpha_prev, weighted_hidden_prev,\n",
    "            input, mask, hUa, W_align, v_align,\n",
    "            W_hid_stacked, W_weightedhid_stacked, W_cell_to_ingate,\n",
    "            W_cell_to_forgetgate, W_cell_to_outgate,\n",
    "            b_stacked, *args):\n",
    "        \n",
    "        sWa = nd.dot(hid_previous, W_align)  # (BS, aln_num_units)\n",
    "        sWa = nd.expand_dims(sWa, axis=1)    # (BS, 1 aln_num_units) \n",
    "        align_act = sWa + hUa\n",
    "        tanh_sWahUa = nd.tanh(align_act)     # (BS, seqlen, num_units_aln)\n",
    "        \n",
    "        # CALCULATE WEIGHT FOR EACH HIDDEN STATE VECTOR\n",
    "        a = nd.dot(tanh_sWahUa, v_align)  # (BS, Seqlen, 1)\n",
    "        a = nd.reshape(a, (a.shape[0], a.shape[1]))\n",
    "        #                                # (BS, Seqlen)\n",
    "        # # ->(BS, seq_len)\n",
    "        \n",
    "        a = a*mask - (1-mask)*10000\n",
    "        \n",
    "        alpha = self.attention_softmax_function(a)\n",
    "        \n",
    "        # input: (BS, Seqlen, num_units)\n",
    "        weighted_hidden = input * nd.expand_dims(alpha, axis=2)\n",
    "        weighted_hidden = nd.sum(weighted_hidden, axis=1)  #sum seqlen out\n",
    "\n",
    "        # (BS, dec_hid) x (dec_hid, dec_hid)\n",
    "        gates = nd.dot(hid_previous, W_hid_stacked) + b_stacked\n",
    "        # (BS, enc_hid) x (enc_hid, dec_hid)\n",
    "        gates = gates + nd.dot(weighted_hidden, W_weightedhid_stacked)\n",
    "\n",
    "        \n",
    "        # Clip gradients\n",
    "        # if self.grad_clipping is not False:\n",
    "        #    gates = theano.gradient.grad_clip(\n",
    "        #        gates, -self.grad_clipping, self.grad_clipping)\n",
    "\n",
    "        # Extract the pre-activation gate values\n",
    "        ingate = self.slice_w(gates, 0)\n",
    "        forgetgate = self.slice_w(gates, 1)\n",
    "        cell_input = self.slice_w(gates, 2)\n",
    "        outgate = self.slice_w(gates, 3)\n",
    "\n",
    "        if self.peepholes:\n",
    "            # Compute peephole connections\n",
    "            ingate = ingate + cell_previous*W_cell_to_ingate\n",
    "            forgetgate = forgetgate + (cell_previous*W_cell_to_forgetgate)\n",
    "            \n",
    "        # Apply nonlinearities\n",
    "        ingate = self.nonlinearity_ingate(ingate)\n",
    "        forgetgate = self.nonlinearity_forgetgate(forgetgate)\n",
    "        cell_input = self.nonlinearity_cell(cell_input)\n",
    "        outgate = self.nonlinearity_outgate(outgate)\n",
    "        \n",
    "        # Compute new cell value\n",
    "        cell = forgetgate*cell_previous + ingate*cell_input\n",
    "        \n",
    "        if self.peepholes:\n",
    "            outgate = outgate + cell*W_cell_to_outgate\n",
    "\n",
    "        # W_align:  (num_units, aln_num_units)\n",
    "        # U_align:  (num_feats, aln_num_units)\n",
    "        # v_align:  (aln_num_units, 1)\n",
    "        # hUa:      (BS, Seqlen, aln_num_units)\n",
    "        # hid:      (BS, num_units_dec)\n",
    "        # input:    (BS, Seqlen, num_inputs)\n",
    "\n",
    "        # Compute new hidden unit activation\n",
    "        hid = outgate*self.nonlinearity_out(cell)\n",
    "\n",
    "        return [cell, hid, alpha, weighted_hidden]            \n",
    "            \n",
    "        \n",
    "    def forward(self, input, mask):\n",
    "        \n",
    "        num_batch = input.shape[0]\n",
    "        encode_seqlen = input.shape[1]\n",
    "        \n",
    "        self.W_hid_to_ingate = self.params.get('W_hid_to_ingate').data()\n",
    "        self.W_hid_to_forgetgate = self.params.get('W_hid_to_forgetgate').data()\n",
    "        self.W_hid_to_cell = self.params.get('W_hid_to_cell').data()\n",
    "        self.W_hid_to_outgate = self.params.get('W_hid_to_outgate').data()\n",
    "        \n",
    "        self.W_weightedhid_to_ingate = self.params.get('W_weightedhid_to_ingate').data()\n",
    "        self.W_weightedhid_to_forgetgate = self.params.get('W_weightedhid_to_forgetgate').data()\n",
    "        self.W_weightedhid_to_cell = self.params.get('W_weightedhid_to_cell').data()\n",
    "        self.W_weightedhid_to_outgate = self.params.get('W_weightedhid_to_outgate').data()\n",
    "        \n",
    "        self.b_ingate = self.params.get('b_ingate').data()\n",
    "        self.b_forgetgate = self.params.get('b_forgetgate').data()\n",
    "        self.b_cell = self.params.get('b_cell').data()\n",
    "        self.b_outgate = self.params.get('b_outgate').data()\n",
    "\n",
    "        W_hid_stacked = nd.concat(\n",
    "            self.W_hid_to_ingate,\n",
    "            self.W_hid_to_forgetgate,\n",
    "            self.W_hid_to_cell,\n",
    "            self.W_hid_to_outgate,\n",
    "            dim=1)\n",
    "        \n",
    "        W_weightedhid_stacked = nd.concat(\n",
    "            self.W_weightedhid_to_ingate,\n",
    "            self.W_weightedhid_to_forgetgate,\n",
    "            self.W_weightedhid_to_cell,\n",
    "            self.W_weightedhid_to_outgate,\n",
    "            dim=1)\n",
    "        \n",
    "        b_stacked = nd.concat(\n",
    "            self.b_ingate,\n",
    "            self.b_forgetgate,\n",
    "            self.b_cell,\n",
    "            self.b_outgate,\n",
    "            dim=0)\n",
    "        \n",
    "        cell = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        hid = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        alpha = nd.zeros((num_batch, encode_seqlen), ctx=ctx)\n",
    "        weighted_hidden = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        \n",
    "        hUa = nd.dot(input, self.params.get('U_align').data())\n",
    "        W_align = self.params.get('W_align').data()\n",
    "        v_align = self.params.get('v_align').data()\n",
    "        \n",
    "        W_cell_to_ingate = self.params.get('W_cell_to_ingate').data()\n",
    "        W_cell_to_forgetgate = self.params.get('W_cell_to_forgetgate').data()\n",
    "        W_cell_to_outgate = self.params.get('W_cell_to_outgate').data()\n",
    "        \n",
    "        for i in range(self.n_decodesteps):        \n",
    "            cell, hid, alpha, weighted_hidden = self.step(cell, hid, alpha, weighted_hidden,\n",
    "                input, mask, hUa, W_align, v_align,\n",
    "                W_hid_stacked, W_weightedhid_stacked, W_cell_to_ingate,\n",
    "                W_cell_to_forgetgate, W_cell_to_outgate,\n",
    "                b_stacked)\n",
    "        \n",
    "        return weighted_hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_dropout_1 = nn.Dropout(rate=drop_per)\n",
    "            self.l_dropout_2 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_dropout_3 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_dropout_4 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_conv = ConvLayer()\n",
    "            self.l_lstm = BidirectionalLSTM()\n",
    "            self.l_dense = nn.Dense(units=n_class, activation='relu')\n",
    "            self.l_decoder = LSTMAttentionDecodeFeedback(num_units=2*n_hid, aln_num_units=n_hid, n_decodesteps=10)\n",
    "    \n",
    "    def forward(self, input, mask):\n",
    "        x = self.l_dropout_1.forward(input)\n",
    "        x = nd.transpose(x, (0, 2, 1))\n",
    "        x = self.l_conv.forward(x)\n",
    "        x = nd.transpose(x, (0, 2, 1))\n",
    "        x = self.l_dropout_2.forward(x)\n",
    "        x = self.l_lstm.forward(x, mask)\n",
    "        x = self.l_decoder(x, mask)\n",
    "        x = self.l_dropout_3.forward(x)\n",
    "        x = self.l_dense.forward(x)\n",
    "        x = self.l_dropout_4.forward(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Time 47.23612 seconds\n",
      "  Train Accuracy: 0.19484\t Train Loss: 2.23418\n",
      "Epoch 2\n",
      "  Time 47.36769 seconds\n",
      "  Train Accuracy: 0.32424\t Train Loss: 1.99028\n",
      "Epoch 3\n",
      "  Time 48.25848 seconds\n",
      "  Train Accuracy: 0.36535\t Train Loss: 1.85189\n",
      "Epoch 4\n",
      "  Time 49.42882 seconds\n",
      "  Train Accuracy: 0.39094\t Train Loss: 1.78757\n",
      "Epoch 5\n",
      "  Time 49.98839 seconds\n",
      "  Train Accuracy: 0.39157\t Train Loss: 1.79086\n",
      "Epoch 6\n",
      "  Time 49.73026 seconds\n",
      "  Train Accuracy: 0.38926\t Train Loss: 1.78958\n",
      "Epoch 7\n",
      "  Time 50.34314 seconds\n",
      "  Train Accuracy: 0.41758\t Train Loss: 1.69893\n",
      "Epoch 8\n",
      "  Time 50.60990 seconds\n",
      "  Train Accuracy: 0.41317\t Train Loss: 1.69375\n",
      "Epoch 9\n",
      "  Time 50.37366 seconds\n",
      "  Train Accuracy: 0.41464\t Train Loss: 1.69628\n",
      "Epoch 10\n",
      "  Time 51.87198 seconds\n",
      "  Train Accuracy: 0.41758\t Train Loss: 1.67987\n",
      "Epoch 11\n",
      "  Time 50.35971 seconds\n",
      "  Train Accuracy: 0.40961\t Train Loss: 1.71810\n",
      "Epoch 12\n",
      "  Time 50.55388 seconds\n",
      "  Train Accuracy: 0.42240\t Train Loss: 1.67449\n",
      "Epoch 13\n",
      "  Time 50.38215 seconds\n",
      "  Train Accuracy: 0.42638\t Train Loss: 1.65757\n",
      "Epoch 14\n",
      "  Time 50.67835 seconds\n",
      "  Train Accuracy: 0.42408\t Train Loss: 1.64360\n",
      "Epoch 15\n",
      "  Time 50.66518 seconds\n",
      "  Train Accuracy: 0.43897\t Train Loss: 1.63198\n",
      "Epoch 16\n",
      "  Time 51.16067 seconds\n",
      "  Train Accuracy: 0.42219\t Train Loss: 1.65507\n",
      "Epoch 17\n",
      "  Time 50.69644 seconds\n",
      "  Train Accuracy: 0.43058\t Train Loss: 1.64181\n",
      "Epoch 18\n",
      "  Time 50.69342 seconds\n",
      "  Train Accuracy: 0.42617\t Train Loss: 1.63677\n",
      "Epoch 19\n",
      "  Time 50.68363 seconds\n",
      "  Train Accuracy: 0.43960\t Train Loss: 1.60732\n",
      "Epoch 20\n",
      "  Time 50.60018 seconds\n",
      "  Train Accuracy: 0.43498\t Train Loss: 1.63203\n",
      "Epoch 21\n",
      "  Time 50.98919 seconds\n",
      "  Train Accuracy: 0.42051\t Train Loss: 1.65146\n",
      "Epoch 22\n",
      "  Time 51.88673 seconds\n",
      "  Train Accuracy: 0.43603\t Train Loss: 1.61448\n",
      "Epoch 23\n",
      "  Time 51.95029 seconds\n",
      "  Train Accuracy: 0.44044\t Train Loss: 1.60666\n",
      "Epoch 24\n",
      "  Time 55.81357 seconds\n",
      "  Train Accuracy: 0.42848\t Train Loss: 1.63363\n",
      "Epoch 25\n",
      "  Time 53.49057 seconds\n",
      "  Train Accuracy: 0.43310\t Train Loss: 1.62036\n",
      "Epoch 26\n",
      "  Time 50.56928 seconds\n",
      "  Train Accuracy: 0.43561\t Train Loss: 1.60341\n",
      "Epoch 27\n",
      "  Time 50.72591 seconds\n",
      "  Train Accuracy: 0.41632\t Train Loss: 1.67390\n",
      "Epoch 28\n",
      "  Time 51.11286 seconds\n",
      "  Train Accuracy: 0.43268\t Train Loss: 1.62992\n",
      "Epoch 29\n",
      "  Time 51.11341 seconds\n",
      "  Train Accuracy: 0.44107\t Train Loss: 1.60055\n",
      "Epoch 30\n",
      "  Time 50.76371 seconds\n",
      "  Train Accuracy: 0.44232\t Train Loss: 1.59340\n",
      "Epoch 31\n",
      "  Time 50.57747 seconds\n",
      "  Train Accuracy: 0.44610\t Train Loss: 1.59406\n",
      "Epoch 32\n",
      "  Time 50.81675 seconds\n",
      "  Train Accuracy: 0.44128\t Train Loss: 1.59107\n",
      "Epoch 33\n",
      "  Time 51.01335 seconds\n",
      "  Train Accuracy: 0.44589\t Train Loss: 1.59540\n",
      "Epoch 34\n",
      "  Time 51.16522 seconds\n",
      "  Train Accuracy: 0.43624\t Train Loss: 1.60121\n",
      "Epoch 35\n",
      "  Time 50.86086 seconds\n",
      "  Train Accuracy: 0.44253\t Train Loss: 1.58733\n",
      "Epoch 36\n",
      "  Time 51.02611 seconds\n",
      "  Train Accuracy: 0.46141\t Train Loss: 1.54595\n",
      "Epoch 37\n",
      "  Time 50.74826 seconds\n",
      "  Train Accuracy: 0.44421\t Train Loss: 1.58086\n",
      "Epoch 38\n",
      "  Time 50.85611 seconds\n",
      "  Train Accuracy: 0.45701\t Train Loss: 1.54932\n",
      "Epoch 39\n",
      "  Time 50.54858 seconds\n",
      "  Train Accuracy: 0.44694\t Train Loss: 1.56879\n",
      "Epoch 40\n",
      "  Time 51.35570 seconds\n",
      "  Train Accuracy: 0.44148\t Train Loss: 1.58664\n",
      "Epoch 41\n",
      "  Time 50.58416 seconds\n",
      "  Train Accuracy: 0.44400\t Train Loss: 1.56903\n",
      "Epoch 42\n",
      "  Time 50.62257 seconds\n",
      "  Train Accuracy: 0.44694\t Train Loss: 1.57736\n",
      "Epoch 43\n",
      "  Time 50.74358 seconds\n",
      "  Train Accuracy: 0.45910\t Train Loss: 1.53903\n",
      "Epoch 44\n",
      "  Time 50.23345 seconds\n",
      "  Train Accuracy: 0.44631\t Train Loss: 1.55864\n",
      "Epoch 45\n",
      "  Time 50.58863 seconds\n",
      "  Train Accuracy: 0.45092\t Train Loss: 1.54645\n",
      "Epoch 46\n",
      "  Time 50.81669 seconds\n",
      "  Train Accuracy: 0.44107\t Train Loss: 1.57253\n",
      "Epoch 47\n",
      "  Time 50.76233 seconds\n",
      "  Train Accuracy: 0.46581\t Train Loss: 1.52682\n",
      "Epoch 48\n",
      "  Time 50.64191 seconds\n",
      "  Train Accuracy: 0.47232\t Train Loss: 1.49455\n",
      "Epoch 49\n",
      "  Time 51.09466 seconds\n",
      "  Train Accuracy: 0.46351\t Train Loss: 1.53550\n",
      "Epoch 50\n",
      "  Time 50.53974 seconds\n",
      "  Train Accuracy: 0.44631\t Train Loss: 1.55965\n",
      "Epoch 51\n",
      "  Time 51.37330 seconds\n",
      "  Train Accuracy: 0.44505\t Train Loss: 1.55668\n",
      "Epoch 52\n",
      "  Time 50.74262 seconds\n",
      "  Train Accuracy: 0.45050\t Train Loss: 1.56434\n",
      "Epoch 53\n",
      "  Time 50.63740 seconds\n",
      "  Train Accuracy: 0.47127\t Train Loss: 1.51624\n",
      "Epoch 54\n",
      "  Time 50.73332 seconds\n",
      "  Train Accuracy: 0.45659\t Train Loss: 1.54580\n",
      "Epoch 55\n",
      "  Time 50.46735 seconds\n",
      "  Train Accuracy: 0.46057\t Train Loss: 1.52159\n",
      "Epoch 56\n",
      "  Time 50.99962 seconds\n",
      "  Train Accuracy: 0.46288\t Train Loss: 1.51775\n",
      "Epoch 57\n",
      "  Time 50.48144 seconds\n",
      "  Train Accuracy: 0.45701\t Train Loss: 1.52209\n",
      "Epoch 58\n",
      "  Time 50.52963 seconds\n",
      "  Train Accuracy: 0.45952\t Train Loss: 1.52837\n",
      "Epoch 59\n",
      "  Time 50.49680 seconds\n",
      "  Train Accuracy: 0.46078\t Train Loss: 1.51227\n",
      "Epoch 60\n",
      "  Time 51.36156 seconds\n",
      "  Train Accuracy: 0.46435\t Train Loss: 1.51213\n",
      "Epoch 61\n",
      "  Time 51.30979 seconds\n",
      "  Train Accuracy: 0.45575\t Train Loss: 1.53172\n",
      "Epoch 62\n",
      "  Time 52.03319 seconds\n",
      "  Train Accuracy: 0.46288\t Train Loss: 1.50920\n",
      "Epoch 63\n",
      "  Time 53.63306 seconds\n",
      "  Train Accuracy: 0.46770\t Train Loss: 1.50892\n",
      "Epoch 64\n",
      "  Time 54.31983 seconds\n",
      "  Train Accuracy: 0.46812\t Train Loss: 1.49540\n",
      "Epoch 65\n",
      "  Time 53.09389 seconds\n",
      "  Train Accuracy: 0.46854\t Train Loss: 1.48700\n",
      "Epoch 66\n",
      "  Time 57.94814 seconds\n",
      "  Train Accuracy: 0.46602\t Train Loss: 1.50526\n",
      "Epoch 67\n",
      "  Time 54.25084 seconds\n",
      "  Train Accuracy: 0.46393\t Train Loss: 1.49749\n",
      "Epoch 68\n",
      "  Time 54.63213 seconds\n",
      "  Train Accuracy: 0.47777\t Train Loss: 1.47212\n",
      "Epoch 69\n",
      "  Time 54.49252 seconds\n",
      "  Train Accuracy: 0.46183\t Train Loss: 1.52002\n",
      "Epoch 70\n",
      "  Time 54.31747 seconds\n",
      "  Train Accuracy: 0.46896\t Train Loss: 1.49810\n",
      "Epoch 71\n",
      "  Time 54.84868 seconds\n",
      "  Train Accuracy: 0.46162\t Train Loss: 1.51786\n",
      "Epoch 72\n",
      "  Time 54.39139 seconds\n",
      "  Train Accuracy: 0.47232\t Train Loss: 1.48795\n",
      "Epoch 73\n",
      "  Time 54.25726 seconds\n",
      "  Train Accuracy: 0.48238\t Train Loss: 1.46968\n",
      "Epoch 74\n",
      "  Time 54.58252 seconds\n",
      "  Train Accuracy: 0.46812\t Train Loss: 1.48692\n",
      "Epoch 75\n",
      "  Time 54.67839 seconds\n",
      "  Train Accuracy: 0.46959\t Train Loss: 1.49147\n",
      "Epoch 76\n",
      "  Time 55.81213 seconds\n",
      "  Train Accuracy: 0.46560\t Train Loss: 1.48945\n",
      "Epoch 77\n",
      "  Time 55.73317 seconds\n",
      "  Train Accuracy: 0.45973\t Train Loss: 1.50325\n",
      "Epoch 78\n",
      "  Time 56.00133 seconds\n",
      "  Train Accuracy: 0.46896\t Train Loss: 1.48984\n",
      "Epoch 79\n",
      "  Time 56.59178 seconds\n",
      "  Train Accuracy: 0.47357\t Train Loss: 1.47702\n",
      "Epoch 80\n",
      "  Time 54.25499 seconds\n",
      "  Train Accuracy: 0.47148\t Train Loss: 1.48525\n",
      "Epoch 81\n",
      "  Time 54.22529 seconds\n",
      "  Train Accuracy: 0.46602\t Train Loss: 1.49990\n",
      "Epoch 82\n",
      "  Time 55.22358 seconds\n",
      "  Train Accuracy: 0.46833\t Train Loss: 1.49215\n",
      "Epoch 83\n",
      "  Time 54.56661 seconds\n",
      "  Train Accuracy: 0.47504\t Train Loss: 1.47399\n",
      "Epoch 84\n",
      "  Time 54.99254 seconds\n",
      "  Train Accuracy: 0.47420\t Train Loss: 1.49334\n",
      "Epoch 85\n",
      "  Time 54.49865 seconds\n",
      "  Train Accuracy: 0.47022\t Train Loss: 1.48353\n",
      "Epoch 86\n",
      "  Time 53.34394 seconds\n",
      "  Train Accuracy: 0.46896\t Train Loss: 1.48430\n",
      "Epoch 87\n",
      "  Time 53.10177 seconds\n",
      "  Train Accuracy: 0.46644\t Train Loss: 1.48262\n",
      "Epoch 88\n",
      "  Time 51.99151 seconds\n",
      "  Train Accuracy: 0.47462\t Train Loss: 1.46959\n",
      "Epoch 89\n",
      "  Time 51.67915 seconds\n",
      "  Train Accuracy: 0.48343\t Train Loss: 1.45056\n",
      "Epoch 90\n",
      "  Time 54.19958 seconds\n",
      "  Train Accuracy: 0.48721\t Train Loss: 1.45176\n",
      "Epoch 91\n",
      "  Time 55.70924 seconds\n",
      "  Train Accuracy: 0.47441\t Train Loss: 1.47046\n",
      "Epoch 92\n",
      "  Time 55.93603 seconds\n",
      "  Train Accuracy: 0.46477\t Train Loss: 1.51895\n",
      "Epoch 93\n",
      "  Time 52.61872 seconds\n",
      "  Train Accuracy: 0.46623\t Train Loss: 1.53387\n",
      "Epoch 94\n",
      "  Time 52.64283 seconds\n",
      "  Train Accuracy: 0.47085\t Train Loss: 1.49387\n",
      "Epoch 95\n",
      "  Time 51.87744 seconds\n",
      "  Train Accuracy: 0.46539\t Train Loss: 1.48850\n",
      "Epoch 96\n",
      "  Time 51.62431 seconds\n",
      "  Train Accuracy: 0.47169\t Train Loss: 1.47904\n",
      "Epoch 97\n",
      "  Time 52.55109 seconds\n",
      "  Train Accuracy: 0.47378\t Train Loss: 1.48409\n",
      "Epoch 98\n",
      "  Time 53.69912 seconds\n",
      "  Train Accuracy: 0.47441\t Train Loss: 1.49329\n",
      "Epoch 99\n",
      "  Time 53.19088 seconds\n",
      "  Train Accuracy: 0.47651\t Train Loss: 1.46686\n",
      "Epoch 100\n",
      "  Time 51.54478 seconds\n",
      "  Train Accuracy: 0.48196\t Train Loss: 1.44935\n",
      "Epoch 101\n",
      "  Time 52.00408 seconds\n",
      "  Train Accuracy: 0.47735\t Train Loss: 1.44666\n",
      "Epoch 102\n",
      "  Time 52.47795 seconds\n",
      "  Train Accuracy: 0.47211\t Train Loss: 1.46923\n",
      "Epoch 103\n",
      "  Time 53.68205 seconds\n",
      "  Train Accuracy: 0.48888\t Train Loss: 1.44063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104\n",
      "  Time 53.09409 seconds\n",
      "  Train Accuracy: 0.47630\t Train Loss: 1.45504\n",
      "Epoch 105\n",
      "  Time 53.76602 seconds\n",
      "  Train Accuracy: 0.47945\t Train Loss: 1.45059\n",
      "Epoch 106\n",
      "  Time 52.38613 seconds\n",
      "  Train Accuracy: 0.48049\t Train Loss: 1.45252\n",
      "Epoch 107\n",
      "  Time 52.74465 seconds\n",
      "  Train Accuracy: 0.47714\t Train Loss: 1.44620\n",
      "Epoch 108\n",
      "  Time 51.35818 seconds\n",
      "  Train Accuracy: 0.48280\t Train Loss: 1.44825\n",
      "Epoch 109\n",
      "  Time 52.33033 seconds\n",
      "  Train Accuracy: 0.47735\t Train Loss: 1.46261\n",
      "Epoch 110\n",
      "  Time 52.65485 seconds\n",
      "  Train Accuracy: 0.48469\t Train Loss: 1.43674\n",
      "Epoch 111\n",
      "  Time 52.55485 seconds\n",
      "  Train Accuracy: 0.48574\t Train Loss: 1.44179\n",
      "Epoch 112\n",
      "  Time 50.53069 seconds\n",
      "  Train Accuracy: 0.47399\t Train Loss: 1.46689\n",
      "Epoch 113\n",
      "  Time 50.43969 seconds\n",
      "  Train Accuracy: 0.48049\t Train Loss: 1.43929\n",
      "Epoch 114\n",
      "  Time 50.87240 seconds\n",
      "  Train Accuracy: 0.49413\t Train Loss: 1.40654\n",
      "Epoch 115\n",
      "  Time 50.58519 seconds\n",
      "  Train Accuracy: 0.47127\t Train Loss: 1.46225\n",
      "Epoch 116\n",
      "  Time 51.02560 seconds\n",
      "  Train Accuracy: 0.48217\t Train Loss: 1.43827\n",
      "Epoch 117\n",
      "  Time 50.81306 seconds\n",
      "  Train Accuracy: 0.48511\t Train Loss: 1.42145\n",
      "Epoch 118\n",
      "  Time 50.78930 seconds\n",
      "  Train Accuracy: 0.47420\t Train Loss: 1.45457\n"
     ]
    }
   ],
   "source": [
    "mask_train = mask_train.as_in_context(ctx)\n",
    "X_train = X_train.as_in_context(ctx)\n",
    "y_train = y_train.as_in_context(ctx)\n",
    "\n",
    "data_iter = mx.io.NDArrayIter([X_train, mask_train], y_train, batch_size, shuffle=True)\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "model = Model()\n",
    "model.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "params = model.collect_params()\n",
    "params.reset_ctx([ctx])\n",
    "\n",
    "trainer = gluon.Trainer(params=params,\n",
    "                        optimizer='adam', optimizer_params={'learning_rate':lr})\n",
    "\n",
    "for e in range(epoch):\n",
    "    begin_time = time.perf_counter()\n",
    "    train_loss = 0.\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iter.reset()\n",
    "    i = -1\n",
    "    for batch in data_iter:\n",
    "        data = batch.data[0]\n",
    "        mask = batch.data[1]\n",
    "        label = batch.label[0]\n",
    "        \n",
    "        with mx.autograd.record():\n",
    "            # output = net(data)\n",
    "            output = model(data, mask)\n",
    "            l = loss(output, label)\n",
    "                \n",
    "        l.backward()\n",
    "        trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        \n",
    "        train_loss += l.mean().asscalar()\n",
    "        preds = output.argmax(axis=1)\n",
    "        acc.update(label, preds)\n",
    "        i += 1\n",
    "    \n",
    "    stop_time = time.perf_counter()\n",
    "    total_time = stop_time - begin_time\n",
    "    print(\"Epoch %d\" % (e + 1))\n",
    "    print(\"  Time %.5f seconds\" % (total_time))\n",
    "    print(\"  Train Accuracy: %.5f\\t Train Loss: %.5f\" % (acc.get()[1], train_loss/(i+1)))\n",
    "\n",
    "model.save_parameters(\"net.params\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_len):\n",
    "    # X: 3-D tensor, valid_len: 1-D or 2-D tensor\n",
    "    if valid_len is None:\n",
    "        return npx.softmax(X)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_len.ndim == 1:\n",
    "            valid_len = valid_len.repeat(shape[1], axis=0)\n",
    "        else:\n",
    "            valid_len = valid_len.reshape(-1)\n",
    "        # Fill masked elements with a large negative, whose exp is 0\n",
    "        X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_len, True,\n",
    "                              axis=1, value=-1e6)\n",
    "        return npx.softmax(X).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.47286\n"
     ]
    }
   ],
   "source": [
    "ctx = mx.cpu(0)\n",
    "\n",
    "test_train = mask_test.as_in_context(ctx)\n",
    "X_test = X_test.as_in_context(ctx)\n",
    "y_test = y_test.as_in_context(ctx)\n",
    "\n",
    "data_iter = mx.io.NDArrayIter([X_test, mask_test], y_test, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# params = model.collect_params()\n",
    "# params.reset_ctx([ctx])\n",
    "\n",
    "model.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "model.load_parameters(\"net.params\", ctx=ctx)\n",
    "\n",
    "acc = mx.metric.Accuracy()\n",
    "\n",
    "for batch in data_iter:\n",
    "    data = batch.data[0]\n",
    "    mask = batch.data[1]\n",
    "    label = batch.label[0]\n",
    "\n",
    "    with mx.autograd.predict_mode():\n",
    "        output = model(data, mask)\n",
    "        \n",
    "    preds = output.argmax(axis=1)\n",
    "    acc.update(label, preds)\n",
    "\n",
    "print(\"Test Accuracy: %.5f\" % (acc.get()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-358a126ff660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "X_test.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
