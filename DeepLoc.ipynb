{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLoc Mxnet Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy\n",
    "import time\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet.initializer as init\n",
    "from mxnet import np, npx, autograd, optimizer, gluon\n",
    "from mxnet.gluon import nn, rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200         #-- integer, epoch\n",
    "batch_size = 32     #-- integer, minibatches size\n",
    "max_seq_size = 1000 #-- integer, maximum sequence size\n",
    "n_hid = 256         #-- integer, number of hidden neurons\n",
    "n_feat = 20         #-- integer, number of features encoded  X_test.shape[2]\n",
    "n_class = 10        #-- integer, number of classes to output\n",
    "lr = 0.0005         #-- float, learning rate\n",
    "drop_per  = 0.2     #-- float, input dropout\n",
    "drop_hid = 0.5      #-- float, hidden neurons dropout\n",
    "n_filt = 10         #-- integer, number of filter in the first convolutional layer\n",
    "seed = 123456       #-- seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "mx.random.seed(seed)\n",
    "npx.random.seed(seed)\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'subcellular_localization/data/train.npz'\n",
    "test_file = 'subcellular_localization/data/test.npz'\n",
    "\n",
    "train_npz = numpy.load(train_file)\n",
    "test_npz = numpy.load(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = nd.from_numpy(train_npz['mask_train'])\n",
    "partition = nd.from_numpy(train_npz['partition'])\n",
    "X_train = nd.from_numpy(train_npz['X_train'])\n",
    "y_train = nd.from_numpy(train_npz['y_train'])\n",
    "X_test = nd.from_numpy(test_npz['X_test'])\n",
    "mask_test = nd.from_numpy(test_npz['mask_test'])\n",
    "y_test = nd.from_numpy(test_npz['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz.close()\n",
    "test_npz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluted Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvLayer, self).__init__(**kwargs)\n",
    "        f_size_a = 1\n",
    "        f_size_b = 3\n",
    "        f_size_c = 5\n",
    "        f_size_d = 9\n",
    "        f_size_e = 15\n",
    "        f_size_f = 21\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.l_conv_a = nn.Conv1D(channels=n_filt, kernel_size=f_size_a, padding=0, layout='NCW', activation='relu')\n",
    "            self.l_conv_b = nn.Conv1D(channels=n_filt, kernel_size=f_size_b, padding=1, layout='NCW', activation='relu')\n",
    "            self.l_conv_c = nn.Conv1D(channels=n_filt, kernel_size=f_size_c, padding=2, layout='NCW', activation='relu')\n",
    "            self.l_conv_d = nn.Conv1D(channels=n_filt, kernel_size=f_size_d, padding=4, layout='NCW', activation='relu')\n",
    "            self.l_conv_e = nn.Conv1D(channels=n_filt, kernel_size=f_size_e, padding=7, layout='NCW', activation='relu')\n",
    "            self.l_conv_f = nn.Conv1D(channels=n_filt, kernel_size=f_size_f, padding=10, layout='NCW', activation='relu')\n",
    "            \n",
    "            self.l_conv_final = nn.Conv1D(channels=64, kernel_size=f_size_b, padding=1, layout='NCW', activation='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.l_conv_a(x)\n",
    "        b = self.l_conv_b(x)\n",
    "        c = self.l_conv_c(x)\n",
    "        d = self.l_conv_d(x)\n",
    "        e = self.l_conv_e(x)\n",
    "        f = self.l_conv_f(x)\n",
    "        \n",
    "        conc = nd.concat(a, b, c, d, e, f, dim=1)\n",
    "        \n",
    "        return self.l_conv_final(conc)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BidirectionalLSTM, self).__init__(**kwargs)\n",
    "        \n",
    "        # self.params.get('mask', shape=(batch_size, max_seq_size))\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_fwd = rnn.LSTM(hidden_size=n_hid, layout='TNC', prefix='LSTMFwd_')\n",
    "            self.l_bck = rnn.LSTM(hidden_size=n_hid, layout='TNC', prefix='LSTMBck_')\n",
    "\n",
    "    def _reverse(self, x, mask):\n",
    "        d = [x, mask]\n",
    "        dataiter = mx.io.NDArrayIter(d)\n",
    "        x_1 = nd.empty(x.shape, ctx=ctx)\n",
    "        nd.reset_arrays(x_1, num_arrays=1)\n",
    "        for i in range(x.shape[0]):\n",
    "            size = nd.sum(mask[i]).astype('int32').asscalar()\n",
    "            seq = x[i]\n",
    "            seq_1 = nd.reverse(nd.slice(seq, begin=(0,0), end=(size, 64)), axis=1)\n",
    "            seq_1.copyto(x_1[i, :size]) \n",
    "        return x_1            \n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        # print('x shape:', x.shape)\n",
    "        # mask = self.params.get('mask').data()\n",
    "        # print('mask:', mask)\n",
    "        dimension = (1, 0, 2)\n",
    "        x_fwd = nd.transpose(x, dimension)\n",
    "        x_bck = nd.transpose(self._reverse(x, mask), dimension)\n",
    "        fwd = self.l_fwd(x_fwd)\n",
    "        bck = self.l_bck(x_bck)\n",
    "        conc = nd.concat(fwd, bck, dim=2)\n",
    "        return nd.transpose(conc, dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicerLayer(nn.Block):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(SlicerLayer, self).__init__(**kwargs)\n",
    "        # self.params.get('mask', shape=(128, 1000))\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # mask = self.params.get('mask').data()\n",
    "        d = [x, mask]\n",
    "        dataiter = mx.io.NDArrayIter(d)\n",
    "        x_shape = x.shape\n",
    "        x_1_shape = (x.shape[0], x.shape[1])\n",
    "        x_1 = nd.empty(x_1_shape, ctx=ctx)\n",
    "        nd.reset_arrays(x_1, num_arrays=1)\n",
    "        for i in range(x_1_shape[0]):\n",
    "            size = nd.sum(mask[i]).astype('int32').asscalar()\n",
    "            x[i][size - 1].copyto(x_1[i])\n",
    "        return x_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionDecodeFeedback(nn.Block):\n",
    "    def __init__(self,\n",
    "                 num_units,\n",
    "                 aln_num_units,\n",
    "                 n_decodesteps=10,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(LSTMAttentionDecodeFeedback, self).__init__(prefix='LSTMAttentinDecode_Feedback_', **kwargs)\n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.aln_num_units = aln_num_units\n",
    "        self.n_decodesteps = n_decodesteps\n",
    "        self.attention_softmax_function = nd.softmax\n",
    "        self.peepholes = True\n",
    "\n",
    "        self.num_inputs = 512\n",
    "        \n",
    "        self.nonlinearity_align=nd.tanh\n",
    "        \n",
    "        self.nonlinearity_ingate = nd.sigmoid\n",
    "        self.nonlinearity_forgetgate = nd.sigmoid\n",
    "        self.nonlinearity_cell = nd.tanh\n",
    "        self.nonlinearity_outgate = nd.sigmoid\n",
    "        \n",
    "        self.nonlinearity_out = nd.tanh\n",
    "        \n",
    "        self.W_hid_to_ingate = self.params.get('W_hid_to_ingate', shape=(num_units, num_units),\n",
    "                                               init=init.Normal(0.1),\n",
    "                                               allow_deferred_init=True)\n",
    "        \n",
    "        self.W_hid_to_forgetgate = self.params.get('W_hid_to_forgetgate', shape=(num_units, num_units),\n",
    "                                                   init=init.Normal(0.1),\n",
    "                                                   allow_deferred_init=True)\n",
    "        \n",
    "        self.W_hid_to_cell = self.params.get('W_hid_to_cell', shape=(num_units, num_units),\n",
    "                                             init=init.Normal(0.1),\n",
    "                                             allow_deferred_init=True)\n",
    "        \n",
    "        self.W_hid_to_outgate = self.params.get('W_hid_to_outgate', shape=(num_units, num_units),\n",
    "                                                init=init.Normal(0.1),\n",
    "                                                allow_deferred_init=True)\n",
    "        \n",
    "        self.b_ingate = self.params.get('b_ingate', shape=(num_units),\n",
    "                                        init=init.Constant(0),\n",
    "                                        allow_deferred_init=True)\n",
    "\n",
    "        self.b_forgetgate = self.params.get('b_forgetgate', shape=(num_units),\n",
    "                                            init=init.Constant(0),\n",
    "                                            allow_deferred_init=True)\n",
    "\n",
    "        self.b_cell = self.params.get('b_cell', shape=(num_units),\n",
    "                                      init=init.Constant(0),\n",
    "                                      allow_deferred_init=True)\n",
    "        \n",
    "        self.b_outgate = self.params.get('b_outgate', shape=(num_units),\n",
    "                                         init=init.Constant(0),\n",
    "                                         allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_ingate = self.params.get('W_weightedhid_to_ingate',\n",
    "                                                      shape=(self.num_inputs, num_units),\n",
    "                                                      init=init.Normal(0.1),\n",
    "                                                      allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_forgetgate = self.params.get('W_weightedhid_to_forgetgate',\n",
    "                                                           shape=(self.num_inputs, num_units),\n",
    "                                                           init=init.Normal(0.1),\n",
    "                                                           allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_cell = self.params.get('W_weightedhid_to_cell',\n",
    "                                                     shape=(self.num_inputs, num_units),\n",
    "                                                     init=init.Normal(0.1),\n",
    "                                                     allow_deferred_init=True)\n",
    "        \n",
    "        self.W_weightedhid_to_outgate = self.params.get('W_weightedhid_to_outgate',\n",
    "                                                        shape=(self.num_inputs, num_units),\n",
    "                                                        init=init.Normal(0.1),\n",
    "                                                        allow_deferred_init=True)\n",
    "        \n",
    "        self.W_cell_to_ingate = self.params.get('W_cell_to_ingate',\n",
    "                                                shape=(num_units),\n",
    "                                                init=init.Normal(0.1),\n",
    "                                                allow_deferred_init=True)\n",
    "        \n",
    "        self.W_cell_to_forgetgate = self.params.get('W_cell_to_forgetgate',\n",
    "                                                    shape=(num_units),\n",
    "                                                    init=init.Normal(0.1),\n",
    "                                                    allow_deferred_init=True)\n",
    "        \n",
    "        self.W_cell_to_outgate = self.params.get('W_cell_to_outgate',\n",
    "                                                 shape=(num_units),\n",
    "                                                 init=init.Normal(0.1),\n",
    "                                                 allow_deferred_init=True)\n",
    "        \n",
    "        self.W_align = self.params.get('W_align',\n",
    "                                       shape=(num_units, self.aln_num_units),\n",
    "                                       init=init.Normal(0.1))\n",
    "        \n",
    "        self.U_align = self.params.get('U_align', shape=(self.num_inputs,self.aln_num_units),\n",
    "                                       init=init.Normal(0.1),\n",
    "                                       allow_deferred_init=True)\n",
    "        \n",
    "        self.v_align = self.params.get('v_align', shape=(self.aln_num_units, 1),\n",
    "                                       init=init.Normal(0.1))\n",
    "        \n",
    "        with self.name_scope():\n",
    "            pass\n",
    "\n",
    "    def slice_w(self, x, n):\n",
    "        return x[:, n*self.num_units:(n+1)*self.num_units]\n",
    "    \n",
    "    def step(self, cell_previous, hid_previous, alpha_prev, weighted_hidden_prev,\n",
    "            input, mask, hUa, W_align, v_align,\n",
    "            W_hid_stacked, W_weightedhid_stacked, W_cell_to_ingate,\n",
    "            W_cell_to_forgetgate, W_cell_to_outgate,\n",
    "            b_stacked, *args):\n",
    "        \n",
    "        sWa = nd.dot(hid_previous, W_align)  # (BS, aln_num_units)\n",
    "        sWa = nd.expand_dims(sWa, axis=1)    # (BS, 1 aln_num_units) \n",
    "        align_act = sWa + hUa\n",
    "        tanh_sWahUa = nd.tanh(align_act)     # (BS, seqlen, num_units_aln)\n",
    "        \n",
    "        # CALCULATE WEIGHT FOR EACH HIDDEN STATE VECTOR\n",
    "        a = nd.dot(tanh_sWahUa, v_align)  # (BS, Seqlen, 1)\n",
    "        a = nd.reshape(a, (a.shape[0], a.shape[1]))\n",
    "        #                                # (BS, Seqlen)\n",
    "        # # ->(BS, seq_len)\n",
    "        \n",
    "        a = a*mask - (1-mask)*10000\n",
    "        \n",
    "        alpha = self.attention_softmax_function(a)\n",
    "        \n",
    "        # input: (BS, Seqlen, num_units)\n",
    "        weighted_hidden = input * nd.expand_dims(alpha, axis=2)\n",
    "        weighted_hidden = nd.sum(weighted_hidden, axis=1)  #sum seqlen out\n",
    "\n",
    "        # (BS, dec_hid) x (dec_hid, dec_hid)\n",
    "        gates = nd.dot(hid_previous, W_hid_stacked) + b_stacked\n",
    "        # (BS, enc_hid) x (enc_hid, dec_hid)\n",
    "        gates = gates + nd.dot(weighted_hidden, W_weightedhid_stacked)\n",
    "\n",
    "        \n",
    "        # Clip gradients\n",
    "        # if self.grad_clipping is not False:\n",
    "        #    gates = theano.gradient.grad_clip(\n",
    "        #        gates, -self.grad_clipping, self.grad_clipping)\n",
    "\n",
    "        # Extract the pre-activation gate values\n",
    "        ingate = self.slice_w(gates, 0)\n",
    "        forgetgate = self.slice_w(gates, 1)\n",
    "        cell_input = self.slice_w(gates, 2)\n",
    "        outgate = self.slice_w(gates, 3)\n",
    "\n",
    "        if self.peepholes:\n",
    "            # Compute peephole connections\n",
    "            ingate = ingate + cell_previous*W_cell_to_ingate\n",
    "            forgetgate = forgetgate + (cell_previous*W_cell_to_forgetgate)\n",
    "            \n",
    "        # Apply nonlinearities\n",
    "        ingate = self.nonlinearity_ingate(ingate)\n",
    "        forgetgate = self.nonlinearity_forgetgate(forgetgate)\n",
    "        cell_input = self.nonlinearity_cell(cell_input)\n",
    "        outgate = self.nonlinearity_outgate(outgate)\n",
    "        \n",
    "        # Compute new cell value\n",
    "        cell = forgetgate*cell_previous + ingate*cell_input\n",
    "        \n",
    "        if self.peepholes:\n",
    "            outgate = outgate + cell*W_cell_to_outgate\n",
    "\n",
    "        # W_align:  (num_units, aln_num_units)\n",
    "        # U_align:  (num_feats, aln_num_units)\n",
    "        # v_align:  (aln_num_units, 1)\n",
    "        # hUa:      (BS, Seqlen, aln_num_units)\n",
    "        # hid:      (BS, num_units_dec)\n",
    "        # input:    (BS, Seqlen, num_inputs)\n",
    "\n",
    "        # Compute new hidden unit activation\n",
    "        hid = outgate*self.nonlinearity_out(cell)\n",
    "\n",
    "        return [cell, hid, alpha, weighted_hidden]            \n",
    "            \n",
    "        \n",
    "    def forward(self, input, mask):\n",
    "        \n",
    "        num_batch = input.shape[0]\n",
    "        encode_seqlen = input.shape[1]\n",
    "        \n",
    "        W_hid_stacked = nd.concat(\n",
    "            self.W_hid_to_ingate.data(),\n",
    "            self.W_hid_to_forgetgate.data(),\n",
    "            self.W_hid_to_cell.data(),\n",
    "            self.W_hid_to_outgate.data(),\n",
    "            dim=1)\n",
    "        \n",
    "        W_weightedhid_stacked = nd.concat(\n",
    "            self.W_weightedhid_to_ingate.data(),\n",
    "            self.W_weightedhid_to_forgetgate.data(),\n",
    "            self.W_weightedhid_to_cell.data(),\n",
    "            self.W_weightedhid_to_outgate.data(),\n",
    "            dim=1)\n",
    "        \n",
    "        b_stacked = nd.concat(\n",
    "            self.b_ingate.data(),\n",
    "            self.b_forgetgate.data(),\n",
    "            self.b_cell.data(),\n",
    "            self.b_outgate.data(),\n",
    "            dim=0)\n",
    "        \n",
    "        cell = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        hid = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        alpha = nd.zeros((num_batch, encode_seqlen), ctx=ctx)\n",
    "        weighted_hidden = nd.zeros((num_batch, self.num_units), ctx=ctx)\n",
    "        \n",
    "        hUa = nd.dot(input, self.U_align.data())\n",
    "        W_align = self.W_align.data()\n",
    "        v_align = self.v_align.data()\n",
    "        \n",
    "        W_cell_to_ingate = self.W_cell_to_ingate.data()\n",
    "        W_cell_to_forgetgate = self.W_cell_to_forgetgate.data()\n",
    "        W_cell_to_outgate = self.W_cell_to_outgate.data()\n",
    "        \n",
    "        for i in range(self.n_decodesteps):        \n",
    "            cell, hid, alpha, weighted_hidden = self.step(cell, hid, alpha, weighted_hidden,\n",
    "                input, mask, hUa, W_align, v_align,\n",
    "                W_hid_stacked, W_weightedhid_stacked, W_cell_to_ingate,\n",
    "                W_cell_to_forgetgate, W_cell_to_outgate,\n",
    "                b_stacked)\n",
    "        \n",
    "        return weighted_hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.l_dropout_1 = nn.Dropout(rate=drop_per)\n",
    "            self.l_dropout_2 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_dropout_3 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_dropout_4 = nn.Dropout(rate=drop_hid)\n",
    "            self.l_conv = ConvLayer()\n",
    "            self.l_lstm = BidirectionalLSTM()\n",
    "            self.l_dense = nn.Dense(units=n_class, activation='relu')\n",
    "            self.l_decoder = LSTMAttentionDecodeFeedback(num_units=2*n_hid, aln_num_units=n_hid, n_decodesteps=10)\n",
    "    \n",
    "    def forward(self, input, mask):\n",
    "        x = self.l_dropout_1.forward(input)\n",
    "        x = nd.transpose(x, (0, 2, 1))\n",
    "        x = self.l_conv.forward(x)\n",
    "        x = nd.transpose(x, (0, 2, 1))\n",
    "        x = self.l_dropout_2.forward(x)\n",
    "        x = self.l_lstm.forward(x, mask)\n",
    "        x = self.l_decoder(x, mask)\n",
    "        x = self.l_dropout_3.forward(x)\n",
    "        x = self.l_dense.forward(x)\n",
    "        x = self.l_dropout_4.forward(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Time 38.59019 seconds\n",
      "  Train Accuracy: 0.19295\t Train Loss: 2.24271\n",
      "Epoch 2\n",
      "  Time 39.44133 seconds\n",
      "  Train Accuracy: 0.26468\t Train Loss: 2.10914\n",
      "Epoch 3\n",
      "  Time 41.44274 seconds\n",
      "  Train Accuracy: 0.34794\t Train Loss: 1.91495\n",
      "Epoch 4\n",
      "  Time 42.32647 seconds\n",
      "  Train Accuracy: 0.36409\t Train Loss: 1.85213\n",
      "Epoch 5\n",
      "  Time 42.10921 seconds\n",
      "  Train Accuracy: 0.34291\t Train Loss: 1.90410\n",
      "Epoch 6\n",
      "  Time 41.94321 seconds\n",
      "  Train Accuracy: 0.36451\t Train Loss: 1.88685\n",
      "Epoch 7\n",
      "  Time 41.13389 seconds\n",
      "  Train Accuracy: 0.40793\t Train Loss: 1.75326\n",
      "Epoch 8\n",
      "  Time 42.25175 seconds\n",
      "  Train Accuracy: 0.40289\t Train Loss: 1.75172\n",
      "Epoch 9\n",
      "  Time 41.45896 seconds\n",
      "  Train Accuracy: 0.40520\t Train Loss: 1.75271\n",
      "Epoch 10\n",
      "  Time 42.25646 seconds\n",
      "  Train Accuracy: 0.38968\t Train Loss: 1.78953\n",
      "Epoch 11\n",
      "  Time 41.79307 seconds\n",
      "  Train Accuracy: 0.36095\t Train Loss: 1.85658\n",
      "Epoch 12\n",
      "  Time 42.19111 seconds\n",
      "  Train Accuracy: 0.35529\t Train Loss: 1.89907\n",
      "Epoch 13\n",
      "  Time 42.30597 seconds\n",
      "  Train Accuracy: 0.39996\t Train Loss: 1.77233\n",
      "Epoch 14\n",
      "  Time 43.15186 seconds\n",
      "  Train Accuracy: 0.40772\t Train Loss: 1.73318\n",
      "Epoch 15\n",
      "  Time 43.39430 seconds\n",
      "  Train Accuracy: 0.41338\t Train Loss: 1.73160\n",
      "Epoch 16\n",
      "  Time 44.44411 seconds\n",
      "  Train Accuracy: 0.40101\t Train Loss: 1.74930\n",
      "Epoch 17\n",
      "  Time 43.84037 seconds\n",
      "  Train Accuracy: 0.39199\t Train Loss: 1.80338\n",
      "Epoch 18\n",
      "  Time 42.42779 seconds\n",
      "  Train Accuracy: 0.40227\t Train Loss: 1.73479\n",
      "Epoch 19\n",
      "  Time 42.40839 seconds\n",
      "  Train Accuracy: 0.42135\t Train Loss: 1.69642\n",
      "Epoch 20\n",
      "  Time 42.58522 seconds\n",
      "  Train Accuracy: 0.41904\t Train Loss: 1.70388\n",
      "Epoch 21\n",
      "  Time 42.34136 seconds\n",
      "  Train Accuracy: 0.40961\t Train Loss: 1.70109\n",
      "Epoch 22\n",
      "  Time 42.41955 seconds\n",
      "  Train Accuracy: 0.42198\t Train Loss: 1.68480\n",
      "Epoch 23\n",
      "  Time 42.50235 seconds\n",
      "  Train Accuracy: 0.42764\t Train Loss: 1.66031\n",
      "Epoch 24\n",
      "  Time 42.15294 seconds\n",
      "  Train Accuracy: 0.41737\t Train Loss: 1.67676\n",
      "Epoch 25\n",
      "  Time 42.39248 seconds\n",
      "  Train Accuracy: 0.42261\t Train Loss: 1.66001\n",
      "Epoch 26\n",
      "  Time 42.90703 seconds\n",
      "  Train Accuracy: 0.42471\t Train Loss: 1.64749\n",
      "Epoch 27\n",
      "  Time 42.79062 seconds\n",
      "  Train Accuracy: 0.41758\t Train Loss: 1.66287\n",
      "Epoch 28\n",
      "  Time 42.45379 seconds\n",
      "  Train Accuracy: 0.42680\t Train Loss: 1.65080\n",
      "Epoch 29\n",
      "  Time 42.53597 seconds\n",
      "  Train Accuracy: 0.43100\t Train Loss: 1.63890\n",
      "Epoch 30\n",
      "  Time 43.15252 seconds\n",
      "  Train Accuracy: 0.43163\t Train Loss: 1.64441\n",
      "Epoch 31\n",
      "  Time 42.58232 seconds\n",
      "  Train Accuracy: 0.42764\t Train Loss: 1.63617\n",
      "Epoch 32\n",
      "  Time 42.67426 seconds\n",
      "  Train Accuracy: 0.43289\t Train Loss: 1.62121\n",
      "Epoch 33\n",
      "  Time 42.59424 seconds\n",
      "  Train Accuracy: 0.43372\t Train Loss: 1.63431\n",
      "Epoch 34\n",
      "  Time 42.58998 seconds\n",
      "  Train Accuracy: 0.42785\t Train Loss: 1.63481\n",
      "Epoch 35\n",
      "  Time 42.56638 seconds\n",
      "  Train Accuracy: 0.43100\t Train Loss: 1.62037\n",
      "Epoch 36\n",
      "  Time 43.75273 seconds\n",
      "  Train Accuracy: 0.36745\t Train Loss: 1.87133\n",
      "Epoch 37\n",
      "  Time 44.58655 seconds\n",
      "  Train Accuracy: 0.36242\t Train Loss: 1.84303\n",
      "Epoch 38\n",
      "  Time 43.80609 seconds\n",
      "  Train Accuracy: 0.41632\t Train Loss: 1.73296\n",
      "Epoch 39\n",
      "  Time 43.99756 seconds\n",
      "  Train Accuracy: 0.41191\t Train Loss: 1.70890\n",
      "Epoch 40\n",
      "  Time 42.85202 seconds\n",
      "  Train Accuracy: 0.41107\t Train Loss: 1.70914\n",
      "Epoch 41\n",
      "  Time 41.94801 seconds\n",
      "  Train Accuracy: 0.41862\t Train Loss: 1.66937\n",
      "Epoch 42\n",
      "  Time 41.85562 seconds\n",
      "  Train Accuracy: 0.41401\t Train Loss: 1.69163\n",
      "Epoch 43\n",
      "  Time 41.89061 seconds\n",
      "  Train Accuracy: 0.42806\t Train Loss: 1.64428\n",
      "Epoch 44\n",
      "  Time 42.36985 seconds\n",
      "  Train Accuracy: 0.41967\t Train Loss: 1.66825\n",
      "Epoch 45\n",
      "  Time 46.17521 seconds\n",
      "  Train Accuracy: 0.42219\t Train Loss: 1.65074\n",
      "Epoch 46\n",
      "  Time 45.01565 seconds\n",
      "  Train Accuracy: 0.41003\t Train Loss: 1.68251\n",
      "Epoch 47\n",
      "  Time 44.66810 seconds\n",
      "  Train Accuracy: 0.43918\t Train Loss: 1.62366\n",
      "Epoch 48\n",
      "  Time 46.06508 seconds\n",
      "  Train Accuracy: 0.44484\t Train Loss: 1.59270\n",
      "Epoch 49\n",
      "  Time 46.93705 seconds\n",
      "  Train Accuracy: 0.43289\t Train Loss: 1.64023\n",
      "Epoch 50\n",
      "  Time 51.13743 seconds\n",
      "  Train Accuracy: 0.42072\t Train Loss: 1.64894\n",
      "Epoch 51\n",
      "  Time 50.13446 seconds\n",
      "  Train Accuracy: 0.41569\t Train Loss: 1.65217\n",
      "Epoch 52\n",
      "  Time 47.33365 seconds\n",
      "  Train Accuracy: 0.41883\t Train Loss: 1.65744\n",
      "Epoch 53\n",
      "  Time 47.67080 seconds\n",
      "  Train Accuracy: 0.43352\t Train Loss: 1.61842\n",
      "Epoch 54\n",
      "  Time 48.15755 seconds\n",
      "  Train Accuracy: 0.42890\t Train Loss: 1.61993\n",
      "Epoch 55\n",
      "  Time 55.02585 seconds\n",
      "  Train Accuracy: 0.43498\t Train Loss: 1.61185\n",
      "Epoch 56\n",
      "  Time 56.95676 seconds\n",
      "  Train Accuracy: 0.43813\t Train Loss: 1.59891\n",
      "Epoch 57\n",
      "  Time 58.84926 seconds\n",
      "  Train Accuracy: 0.43310\t Train Loss: 1.62114\n",
      "Epoch 58\n",
      "  Time 58.59788 seconds\n",
      "  Train Accuracy: 0.43477\t Train Loss: 1.60902\n",
      "Epoch 59\n",
      "  Time 58.15042 seconds\n",
      "  Train Accuracy: 0.43645\t Train Loss: 1.59195\n",
      "Epoch 60\n",
      "  Time 52.38961 seconds\n",
      "  Train Accuracy: 0.43687\t Train Loss: 1.59431\n",
      "Epoch 61\n",
      "  Time 47.72185 seconds\n",
      "  Train Accuracy: 0.42366\t Train Loss: 1.64994\n",
      "Epoch 62\n",
      "  Time 47.73873 seconds\n",
      "  Train Accuracy: 0.43352\t Train Loss: 1.62022\n",
      "Epoch 63\n",
      "  Time 47.31717 seconds\n",
      "  Train Accuracy: 0.43876\t Train Loss: 1.59932\n",
      "Epoch 64\n",
      "  Time 47.08899 seconds\n",
      "  Train Accuracy: 0.44484\t Train Loss: 1.58007\n",
      "Epoch 65\n",
      "  Time 49.80980 seconds\n",
      "  Train Accuracy: 0.44107\t Train Loss: 1.57540\n",
      "Epoch 66\n",
      "  Time 48.79215 seconds\n",
      "  Train Accuracy: 0.44379\t Train Loss: 1.58729\n",
      "Epoch 67\n",
      "  Time 48.30852 seconds\n",
      "  Train Accuracy: 0.44065\t Train Loss: 1.57630\n",
      "Epoch 68\n",
      "  Time 49.13580 seconds\n",
      "  Train Accuracy: 0.44128\t Train Loss: 1.60785\n",
      "Epoch 69\n",
      "  Time 48.10735 seconds\n",
      "  Train Accuracy: 0.42282\t Train Loss: 1.64562\n",
      "Epoch 70\n",
      "  Time 48.43394 seconds\n",
      "  Train Accuracy: 0.43750\t Train Loss: 1.59236\n",
      "Epoch 71\n",
      "  Time 48.44257 seconds\n",
      "  Train Accuracy: 0.42345\t Train Loss: 1.63913\n",
      "Epoch 72\n",
      "  Time 48.40240 seconds\n",
      "  Train Accuracy: 0.43624\t Train Loss: 1.61712\n",
      "Epoch 73\n",
      "  Time 48.79038 seconds\n",
      "  Train Accuracy: 0.45428\t Train Loss: 1.56662\n",
      "Epoch 74\n",
      "  Time 48.51022 seconds\n",
      "  Train Accuracy: 0.43960\t Train Loss: 1.59056\n",
      "Epoch 75\n",
      "  Time 51.33223 seconds\n",
      "  Train Accuracy: 0.44211\t Train Loss: 1.57127\n",
      "Epoch 76\n",
      "  Time 48.10061 seconds\n",
      "  Train Accuracy: 0.44169\t Train Loss: 1.56180\n",
      "Epoch 77\n",
      "  Time 48.43341 seconds\n",
      "  Train Accuracy: 0.43624\t Train Loss: 1.58589\n",
      "Epoch 78\n",
      "  Time 48.73490 seconds\n",
      "  Train Accuracy: 0.43834\t Train Loss: 1.57396\n",
      "Epoch 79\n",
      "  Time 48.15162 seconds\n",
      "  Train Accuracy: 0.44107\t Train Loss: 1.57300\n",
      "Epoch 80\n",
      "  Time 50.71002 seconds\n",
      "  Train Accuracy: 0.44400\t Train Loss: 1.58004\n",
      "Epoch 81\n",
      "  Time 49.33023 seconds\n",
      "  Train Accuracy: 0.44400\t Train Loss: 1.57568\n",
      "Epoch 82\n",
      "  Time 48.28627 seconds\n",
      "  Train Accuracy: 0.44086\t Train Loss: 1.56590\n",
      "Epoch 83\n",
      "  Time 49.48803 seconds\n",
      "  Train Accuracy: 0.45302\t Train Loss: 1.54777\n",
      "Epoch 84\n",
      "  Time 50.43612 seconds\n",
      "  Train Accuracy: 0.44652\t Train Loss: 1.57015\n",
      "Epoch 85\n",
      "  Time 48.33585 seconds\n",
      "  Train Accuracy: 0.44736\t Train Loss: 1.56060\n",
      "Epoch 86\n",
      "  Time 49.96337 seconds\n",
      "  Train Accuracy: 0.44463\t Train Loss: 1.56439\n",
      "Epoch 87\n",
      "  Time 48.78023 seconds\n",
      "  Train Accuracy: 0.44337\t Train Loss: 1.56137\n",
      "Epoch 88\n",
      "  Time 48.74769 seconds\n",
      "  Train Accuracy: 0.45365\t Train Loss: 1.54538\n",
      "Epoch 89\n",
      "  Time 47.93455 seconds\n",
      "  Train Accuracy: 0.45533\t Train Loss: 1.54328\n",
      "Epoch 90\n",
      "  Time 48.80070 seconds\n",
      "  Train Accuracy: 0.46477\t Train Loss: 1.52742\n",
      "Epoch 91\n",
      "  Time 47.74475 seconds\n",
      "  Train Accuracy: 0.45218\t Train Loss: 1.54678\n",
      "Epoch 92\n",
      "  Time 47.90747 seconds\n",
      "  Train Accuracy: 0.44547\t Train Loss: 1.55315\n",
      "Epoch 93\n",
      "  Time 48.63933 seconds\n",
      "  Train Accuracy: 0.45784\t Train Loss: 1.54456\n",
      "Epoch 94\n",
      "  Time 48.14631 seconds\n",
      "  Train Accuracy: 0.45428\t Train Loss: 1.53713\n",
      "Epoch 95\n",
      "  Time 48.86677 seconds\n",
      "  Train Accuracy: 0.45092\t Train Loss: 1.52652\n",
      "Epoch 96\n",
      "  Time 48.68197 seconds\n",
      "  Train Accuracy: 0.45470\t Train Loss: 1.53636\n",
      "Epoch 97\n",
      "  Time 48.89098 seconds\n",
      "  Train Accuracy: 0.45617\t Train Loss: 1.53710\n",
      "Epoch 98\n",
      "  Time 48.58595 seconds\n",
      "  Train Accuracy: 0.45302\t Train Loss: 1.55893\n",
      "Epoch 99\n",
      "  Time 48.60506 seconds\n",
      "  Train Accuracy: 0.45092\t Train Loss: 1.54458\n",
      "Epoch 100\n",
      "  Time 48.50471 seconds\n",
      "  Train Accuracy: 0.46456\t Train Loss: 1.51357\n",
      "Epoch 101\n",
      "  Time 51.25346 seconds\n",
      "  Train Accuracy: 0.45470\t Train Loss: 1.52991\n",
      "Epoch 102\n",
      "  Time 48.29223 seconds\n",
      "  Train Accuracy: 0.45784\t Train Loss: 1.52616\n",
      "Epoch 103\n",
      "  Time 52.21529 seconds\n",
      "  Train Accuracy: 0.46728\t Train Loss: 1.51758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104\n",
      "  Time 50.86430 seconds\n",
      "  Train Accuracy: 0.45805\t Train Loss: 1.52226\n",
      "Epoch 105\n",
      "  Time 51.83350 seconds\n",
      "  Train Accuracy: 0.45742\t Train Loss: 1.52197\n",
      "Epoch 106\n",
      "  Time 48.55796 seconds\n",
      "  Train Accuracy: 0.46057\t Train Loss: 1.51827\n",
      "Epoch 107\n",
      "  Time 48.28434 seconds\n",
      "  Train Accuracy: 0.45931\t Train Loss: 1.50955\n",
      "Epoch 108\n",
      "  Time 49.02268 seconds\n",
      "  Train Accuracy: 0.46497\t Train Loss: 1.51534\n",
      "Epoch 109\n",
      "  Time 47.78355 seconds\n",
      "  Train Accuracy: 0.45784\t Train Loss: 1.53680\n",
      "Epoch 110\n",
      "  Time 48.45295 seconds\n",
      "  Train Accuracy: 0.46644\t Train Loss: 1.50516\n",
      "Epoch 111\n",
      "  Time 47.47860 seconds\n",
      "  Train Accuracy: 0.46162\t Train Loss: 1.53554\n",
      "Epoch 112\n",
      "  Time 47.54410 seconds\n",
      "  Train Accuracy: 0.43184\t Train Loss: 1.60476\n",
      "Epoch 113\n",
      "  Time 49.33185 seconds\n",
      "  Train Accuracy: 0.43729\t Train Loss: 1.61509\n",
      "Epoch 114\n",
      "  Time 48.33796 seconds\n",
      "  Train Accuracy: 0.46015\t Train Loss: 1.53342\n",
      "Epoch 115\n",
      "  Time 47.79952 seconds\n",
      "  Train Accuracy: 0.42408\t Train Loss: 1.62822\n",
      "Epoch 116\n",
      "  Time 49.09722 seconds\n",
      "  Train Accuracy: 0.44568\t Train Loss: 1.57733\n",
      "Epoch 117\n",
      "  Time 48.99327 seconds\n",
      "  Train Accuracy: 0.45784\t Train Loss: 1.53698\n",
      "Epoch 118\n",
      "  Time 47.79989 seconds\n",
      "  Train Accuracy: 0.44841\t Train Loss: 1.55760\n",
      "Epoch 119\n",
      "  Time 53.19531 seconds\n",
      "  Train Accuracy: 0.42198\t Train Loss: 1.66321\n",
      "Epoch 120\n",
      "  Time 48.92940 seconds\n",
      "  Train Accuracy: 0.43792\t Train Loss: 1.61074\n",
      "Epoch 121\n",
      "  Time 49.07666 seconds\n",
      "  Train Accuracy: 0.45596\t Train Loss: 1.57318\n",
      "Epoch 122\n",
      "  Time 51.44283 seconds\n",
      "  Train Accuracy: 0.45197\t Train Loss: 1.54570\n",
      "Epoch 123\n",
      "  Time 48.34816 seconds\n",
      "  Train Accuracy: 0.45197\t Train Loss: 1.55627\n",
      "Epoch 124\n",
      "  Time 47.83965 seconds\n",
      "  Train Accuracy: 0.42995\t Train Loss: 1.62540\n",
      "Epoch 125\n",
      "  Time 48.09761 seconds\n",
      "  Train Accuracy: 0.40562\t Train Loss: 1.72950\n",
      "Epoch 126\n",
      "  Time 49.32426 seconds\n",
      "  Train Accuracy: 0.43624\t Train Loss: 1.61946\n",
      "Epoch 127\n",
      "  Time 50.15505 seconds\n",
      "  Train Accuracy: 0.42534\t Train Loss: 1.63207\n",
      "Epoch 128\n",
      "  Time 48.80201 seconds\n",
      "  Train Accuracy: 0.44128\t Train Loss: 1.58616\n",
      "Epoch 129\n",
      "  Time 49.91326 seconds\n",
      "  Train Accuracy: 0.45155\t Train Loss: 1.54303\n",
      "Epoch 130\n",
      "  Time 48.81383 seconds\n",
      "  Train Accuracy: 0.43477\t Train Loss: 1.59568\n",
      "Epoch 131\n",
      "  Time 48.51405 seconds\n",
      "  Train Accuracy: 0.42701\t Train Loss: 1.61619\n",
      "Epoch 132\n",
      "  Time 47.59120 seconds\n",
      "  Train Accuracy: 0.42135\t Train Loss: 1.64223\n",
      "Epoch 133\n",
      "  Time 49.00720 seconds\n",
      "  Train Accuracy: 0.44023\t Train Loss: 1.58687\n",
      "Epoch 134\n",
      "  Time 49.72708 seconds\n",
      "  Train Accuracy: 0.44295\t Train Loss: 1.59659\n",
      "Epoch 135\n",
      "  Time 62.10511 seconds\n",
      "  Train Accuracy: 0.44820\t Train Loss: 1.55735\n",
      "Epoch 136\n",
      "  Time 51.60942 seconds\n",
      "  Train Accuracy: 0.41737\t Train Loss: 1.68789\n",
      "Epoch 137\n",
      "  Time 48.90498 seconds\n",
      "  Train Accuracy: 0.43016\t Train Loss: 1.62313\n",
      "Epoch 138\n",
      "  Time 48.44365 seconds\n",
      "  Train Accuracy: 0.43834\t Train Loss: 1.60143\n",
      "Epoch 139\n",
      "  Time 48.23796 seconds\n",
      "  Train Accuracy: 0.43477\t Train Loss: 1.59325\n",
      "Epoch 140\n",
      "  Time 47.32962 seconds\n",
      "  Train Accuracy: 0.44841\t Train Loss: 1.56643\n",
      "Epoch 141\n",
      "  Time 47.29294 seconds\n",
      "  Train Accuracy: 0.44379\t Train Loss: 1.56842\n",
      "Epoch 142\n",
      "  Time 47.30920 seconds\n",
      "  Train Accuracy: 0.44107\t Train Loss: 1.56241\n",
      "Epoch 143\n",
      "  Time 47.50659 seconds\n",
      "  Train Accuracy: 0.43981\t Train Loss: 1.61632\n",
      "Epoch 144\n",
      "  Time 47.35282 seconds\n",
      "  Train Accuracy: 0.43121\t Train Loss: 1.62122\n",
      "Epoch 145\n",
      "  Time 47.21141 seconds\n",
      "  Train Accuracy: 0.44610\t Train Loss: 1.59050\n",
      "Epoch 146\n",
      "  Time 47.21271 seconds\n",
      "  Train Accuracy: 0.44065\t Train Loss: 1.59700\n",
      "Epoch 147\n",
      "  Time 47.24118 seconds\n",
      "  Train Accuracy: 0.45512\t Train Loss: 1.54861\n",
      "Epoch 148\n",
      "  Time 47.20541 seconds\n",
      "  Train Accuracy: 0.44778\t Train Loss: 1.57368\n",
      "Epoch 149\n",
      "  Time 46.93050 seconds\n",
      "  Train Accuracy: 0.45596\t Train Loss: 1.54570\n",
      "Epoch 150\n",
      "  Time 46.99169 seconds\n",
      "  Train Accuracy: 0.45008\t Train Loss: 1.55405\n",
      "Epoch 151\n",
      "  Time 47.26351 seconds\n",
      "  Train Accuracy: 0.45742\t Train Loss: 1.53227\n",
      "Epoch 152\n",
      "  Time 47.10446 seconds\n",
      "  Train Accuracy: 0.45701\t Train Loss: 1.52933\n",
      "Epoch 153\n",
      "  Time 48.66618 seconds\n",
      "  Train Accuracy: 0.47022\t Train Loss: 1.50651\n",
      "Epoch 154\n",
      "  Time 47.18301 seconds\n",
      "  Train Accuracy: 0.45533\t Train Loss: 1.55096\n",
      "Epoch 155\n",
      "  Time 47.17862 seconds\n",
      "  Train Accuracy: 0.45155\t Train Loss: 1.55523\n",
      "Epoch 156\n",
      "  Time 47.37789 seconds\n",
      "  Train Accuracy: 0.44463\t Train Loss: 1.54665\n",
      "Epoch 157\n",
      "  Time 47.13648 seconds\n",
      "  Train Accuracy: 0.44945\t Train Loss: 1.54240\n",
      "Epoch 158\n",
      "  Time 47.32899 seconds\n",
      "  Train Accuracy: 0.46309\t Train Loss: 1.52202\n",
      "Epoch 159\n",
      "  Time 47.18522 seconds\n",
      "  Train Accuracy: 0.45805\t Train Loss: 1.51643\n",
      "Epoch 160\n",
      "  Time 47.24099 seconds\n",
      "  Train Accuracy: 0.45847\t Train Loss: 1.50655\n",
      "Epoch 161\n",
      "  Time 47.13973 seconds\n",
      "  Train Accuracy: 0.46288\t Train Loss: 1.50709\n",
      "Epoch 162\n",
      "  Time 47.00429 seconds\n",
      "  Train Accuracy: 0.45805\t Train Loss: 1.51511\n",
      "Epoch 163\n",
      "  Time 47.18956 seconds\n",
      "  Train Accuracy: 0.46036\t Train Loss: 1.51361\n",
      "Epoch 164\n",
      "  Time 47.10714 seconds\n",
      "  Train Accuracy: 0.46581\t Train Loss: 1.51269\n",
      "Epoch 165\n",
      "  Time 46.92842 seconds\n",
      "  Train Accuracy: 0.46812\t Train Loss: 1.49561\n",
      "Epoch 166\n",
      "  Time 47.31319 seconds\n",
      "  Train Accuracy: 0.47211\t Train Loss: 1.49946\n",
      "Epoch 167\n",
      "  Time 46.99107 seconds\n",
      "  Train Accuracy: 0.46456\t Train Loss: 1.50429\n",
      "Epoch 168\n",
      "  Time 46.97479 seconds\n",
      "  Train Accuracy: 0.45931\t Train Loss: 1.53282\n",
      "Epoch 169\n",
      "  Time 46.99409 seconds\n",
      "  Train Accuracy: 0.47756\t Train Loss: 1.48030\n",
      "Epoch 170\n",
      "  Time 47.32555 seconds\n",
      "  Train Accuracy: 0.46833\t Train Loss: 1.50549\n",
      "Epoch 171\n",
      "  Time 47.14276 seconds\n",
      "  Train Accuracy: 0.46204\t Train Loss: 1.50249\n",
      "Epoch 172\n",
      "  Time 47.23579 seconds\n",
      "  Train Accuracy: 0.48511\t Train Loss: 1.46974\n",
      "Epoch 173\n",
      "  Time 46.83160 seconds\n",
      "  Train Accuracy: 0.46414\t Train Loss: 1.50785\n",
      "Epoch 174\n",
      "  Time 46.97991 seconds\n",
      "  Train Accuracy: 0.44966\t Train Loss: 1.54285\n",
      "Epoch 175\n",
      "  Time 46.99835 seconds\n",
      "  Train Accuracy: 0.45344\t Train Loss: 1.53527\n",
      "Epoch 176\n",
      "  Time 46.99317 seconds\n",
      "  Train Accuracy: 0.47022\t Train Loss: 1.48758\n",
      "Epoch 177\n",
      "  Time 46.76697 seconds\n",
      "  Train Accuracy: 0.46351\t Train Loss: 1.51245\n",
      "Epoch 178\n",
      "  Time 46.90452 seconds\n",
      "  Train Accuracy: 0.46120\t Train Loss: 1.51571\n",
      "Epoch 179\n",
      "  Time 47.35228 seconds\n",
      "  Train Accuracy: 0.46623\t Train Loss: 1.48469\n",
      "Epoch 180\n",
      "  Time 47.06918 seconds\n",
      "  Train Accuracy: 0.46309\t Train Loss: 1.51026\n",
      "Epoch 181\n",
      "  Time 47.11097 seconds\n",
      "  Train Accuracy: 0.47001\t Train Loss: 1.49654\n",
      "Epoch 182\n",
      "  Time 47.18926 seconds\n",
      "  Train Accuracy: 0.46854\t Train Loss: 1.49338\n",
      "Epoch 183\n",
      "  Time 47.05037 seconds\n",
      "  Train Accuracy: 0.47441\t Train Loss: 1.48068\n",
      "Epoch 184\n",
      "  Time 47.15036 seconds\n",
      "  Train Accuracy: 0.48196\t Train Loss: 1.46153\n",
      "Epoch 185\n",
      "  Time 47.13959 seconds\n",
      "  Train Accuracy: 0.46980\t Train Loss: 1.48440\n",
      "Epoch 186\n",
      "  Time 46.96606 seconds\n",
      "  Train Accuracy: 0.46518\t Train Loss: 1.49062\n",
      "Epoch 187\n",
      "  Time 47.00003 seconds\n",
      "  Train Accuracy: 0.46414\t Train Loss: 1.49131\n",
      "Epoch 188\n",
      "  Time 46.99122 seconds\n",
      "  Train Accuracy: 0.47336\t Train Loss: 1.46437\n",
      "Epoch 189\n",
      "  Time 46.92100 seconds\n",
      "  Train Accuracy: 0.47273\t Train Loss: 1.46802\n",
      "Epoch 190\n",
      "  Time 47.11753 seconds\n",
      "  Train Accuracy: 0.46497\t Train Loss: 1.49840\n",
      "Epoch 191\n",
      "  Time 46.90757 seconds\n",
      "  Train Accuracy: 0.47693\t Train Loss: 1.46416\n",
      "Epoch 192\n",
      "  Time 46.96164 seconds\n",
      "  Train Accuracy: 0.47043\t Train Loss: 1.47929\n",
      "Epoch 193\n",
      "  Time 47.05587 seconds\n",
      "  Train Accuracy: 0.46875\t Train Loss: 1.48784\n",
      "Epoch 194\n",
      "  Time 47.01055 seconds\n",
      "  Train Accuracy: 0.47630\t Train Loss: 1.48813\n",
      "Epoch 195\n",
      "  Time 46.93045 seconds\n",
      "  Train Accuracy: 0.46833\t Train Loss: 1.48199\n",
      "Epoch 196\n",
      "  Time 46.64373 seconds\n",
      "  Train Accuracy: 0.48553\t Train Loss: 1.45975\n",
      "Epoch 197\n",
      "  Time 47.02482 seconds\n",
      "  Train Accuracy: 0.46309\t Train Loss: 1.49438\n",
      "Epoch 198\n",
      "  Time 46.87724 seconds\n",
      "  Train Accuracy: 0.46875\t Train Loss: 1.48336\n",
      "Epoch 199\n",
      "  Time 46.92193 seconds\n",
      "  Train Accuracy: 0.48322\t Train Loss: 1.44804\n",
      "Epoch 200\n",
      "  Time 46.91677 seconds\n",
      "  Train Accuracy: 0.47483\t Train Loss: 1.48076\n"
     ]
    }
   ],
   "source": [
    "mask_train = mask_train.as_in_context(ctx)\n",
    "X_train = X_train.as_in_context(ctx)\n",
    "y_train = y_train.as_in_context(ctx)\n",
    "\n",
    "data_iter = mx.io.NDArrayIter([X_train, mask_train], y_train, batch_size, shuffle=True)\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "model = Model()\n",
    "model.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "params = model.collect_params()\n",
    "params.reset_ctx([ctx])\n",
    "\n",
    "trainer = gluon.Trainer(params=params,\n",
    "                        optimizer='adam', optimizer_params={'learning_rate':lr})\n",
    "\n",
    "for e in range(epoch):\n",
    "    begin_time = time.perf_counter()\n",
    "    train_loss = 0.\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iter.reset()\n",
    "    i = -1\n",
    "    for batch in data_iter:\n",
    "        input = batch.data[0]\n",
    "        mask = batch.data[1]\n",
    "        label = batch.label[0]\n",
    "        \n",
    "        with mx.autograd.record():\n",
    "            output = model(input, mask)\n",
    "            l = loss(output, label)\n",
    "                \n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_loss += l.mean().asscalar()\n",
    "        preds = output.argmax(axis=1)\n",
    "        acc.update(label, preds)\n",
    "        i += 1\n",
    "    \n",
    "    stop_time = time.perf_counter()\n",
    "    total_time = stop_time - begin_time\n",
    "    print(\"Epoch %d\" % (e + 1))\n",
    "    print(\"  Time %.5f seconds\" % (total_time))\n",
    "    print(\"  Train Accuracy: %.5f\\t Train Loss: %.5f\" % (acc.get()[1], train_loss/(i+1)))\n",
    "\n",
    "model.save_parameters(\"net.params\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.78372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mask_test = mask_test.as_in_context(ctx)\n",
    "X_test = X_test.as_in_context(ctx)\n",
    "y_test = y_test.as_in_context(ctx)\n",
    "\n",
    "data_iter = mx.io.NDArrayIter([X_test, mask_test], y_test, batch_size, shuffle=False)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.load_parameters(\"net.params\", ctx=ctx)\n",
    "\n",
    "acc = mx.metric.Accuracy()\n",
    "\n",
    "for batch in data_iter:\n",
    "    input = batch.data[0]\n",
    "    mask = batch.data[1]\n",
    "    label = batch.label[0]\n",
    "\n",
    "    with mx.autograd.predict_mode():\n",
    "        output = model(input, mask)\n",
    "        \n",
    "    preds = output.argmax(axis=1)\n",
    "    acc.update(label, preds)\n",
    "\n",
    "print(\"Test Accuracy: %.5f\" % (acc.get()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
